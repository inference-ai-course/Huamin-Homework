# Cell 1: å¯¼å…¥å’Œé…ç½®
import sqlite3
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from fastapi import FastAPI
import uvicorn
from pathlib import Path
import re
from datetime import datetime
from contextlib import asynccontextmanager
import nest_asyncio
import requests
from typing import List, Dict
import json
import glob

# ä¿®å¤äº‹ä»¶å¾ªç¯é—®é¢˜
nest_asyncio.apply()

# -----------------------------
# å…¨å±€é…ç½®
# -----------------------------
PDF_DIR = Path("arxiv_pdfs")
TXT_DIR = Path("arxiv_texts")  # ä½¿ç”¨week4å·²ç»ç”Ÿæˆçš„æ–‡æœ¬æ–‡ä»¶
DB_PATH = "hybrid_search.db"
MODEL_NAME = "all-MiniLM-L6-v2"

# ç¡®ä¿ç›®å½•å­˜åœ¨
PDF_DIR.mkdir(exist_ok=True)
TXT_DIR.mkdir(exist_ok=True)

# Cell 2: æ–‡æœ¬å¤„ç†å‡½æ•°
def extract_metadata_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–å…ƒæ•°æ®"""
    # ç§»é™¤æ–‡ä»¶æ‰©å±•å
    name_without_ext = os.path.splitext(filename)[0]
    
    # æ ¹æ®å®é™…æ–‡ä»¶åæ ¼å¼è¿›è¡Œè°ƒæ•´
    
    # ç¤ºä¾‹ï¼šç®€å•çš„åˆ†å‰²æ–¹å¼
    parts = name_without_ext.split('_')
    
    metadata = {
        "title": parts[0] if len(parts) > 0 else "æœªçŸ¥æ ‡é¢˜",
        "authors": parts[1] if len(parts) > 1 else "æœªçŸ¥ä½œè€…",
        "year": int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 2023,
        "abstract": "",  # é»˜è®¤ç©ºæ‘˜è¦
        "keywords": ""   # é»˜è®¤ç©ºå…³é”®è¯
    }
    
    return metadata

def load_text_from_file(txt_file: Path) -> str:
    """ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½å†…å®¹"""
    try:
        with open(txt_file, 'r', encoding='utf-8') as f:
            return f.read()
    except:
        return ""

def chunk_text(doc_id: str, text: str, chunk_size=500, overlap=100) -> list:
    """æ–‡æœ¬åˆ†å—"""
    words = text.split()
    chunks = []
    i = 0
    chunk_id = 0
    while i < len(words):
        chunk_words = words[i:i + chunk_size]
        chunk_text = " ".join(chunk_words)
        chunks.append({
            "doc_id": doc_id,
            "chunk_id": chunk_id,
            "text": chunk_text
        })
        i += chunk_size - overlap
        chunk_id += 1
    return chunks

# Cell 3: æ•°æ®åº“åˆå§‹åŒ–
def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“ï¼Œåˆ›å»ºå¿…è¦çš„è¡¨"""
    conn = sqlite3.connect('document_search.db')
    
    # åˆ›å»ºæ–‡æ¡£è¡¨ - ä¿®æ­£ä¸ºä¸æ’å…¥è¯­å¥åŒ¹é…çš„ç»“æ„
    conn.execute("""
    CREATE TABLE IF NOT EXISTS documents (
        doc_id TEXT PRIMARY KEY,
        title TEXT NOT NULL,
        authors TEXT,
        year INTEGER,
        abstract TEXT,
        keywords TEXT,
        file_path TEXT NOT NULL,
        upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """)
    
    # åˆ›å»ºFTS5è™šæ‹Ÿè¡¨ç”¨äºå…¨æ–‡æœç´¢
    conn.execute("""
    CREATE VIRTUAL TABLE IF NOT EXISTS doc_chunks USING fts5(
        chunk_id UNINDEXED,
        content,
        doc_id UNINDEXED,
        title
    );
    """)
    
    conn.commit()
    print("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼")
    return conn

# Cell 4: æ•°æ®åº“å¡«å……å‡½æ•°
def split_text_into_chunks(text, chunk_size=512, overlap=50):
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆé‡å çš„å—
    """
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = start + chunk_size
        if end > text_length:
            end = text_length
        
        chunk = text[start:end]
        chunks.append(chunk)
        
        # ç§»åŠ¨èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å 
        start += chunk_size - overlap
        
        # ç¡®ä¿ä¸ä¼šæ— é™å¾ªç¯
        if start >= text_length:
            break
    
    return chunks

def populate_database(conn):
    """ä»æ–‡æœ¬æ–‡ä»¶å¡«å……æ•°æ®åº“"""
    cursor = conn.cursor()
    
    # è·å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶
    txt_files = glob.glob(os.path.join(TXT_DIR, "*.txt"))
    print(f"æ‰¾åˆ° {len(txt_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
    
    for txt_file in txt_files:
        # ä»æ–‡ä»¶åæå–æ–‡æ¡£IDï¼ˆä¸å«æ‰©å±•åï¼‰
        doc_id = os.path.splitext(os.path.basename(txt_file))[0]
        
        # ä»æ–‡ä»¶åæå–å…ƒæ•°æ®
        metadata = extract_metadata_from_filename(doc_id)
        
        # æ’å…¥æ–‡æ¡£å…ƒæ•°æ® - æ·»åŠ  file_path å­—æ®µ
        cursor.execute("""
            INSERT OR REPLACE INTO documents (doc_id, title, authors, year, abstract, keywords, file_path)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (doc_id, metadata["title"], metadata["authors"], 
              metadata["year"], metadata["abstract"], "", txt_file))  # æ·»åŠ  txt_file ä½œä¸º file_path
        
        # åŠ è½½æ–‡æœ¬å†…å®¹
        text = load_text_from_file(txt_file)
        
        # åˆ†å‰²æ–‡æœ¬ä¸ºå—
        chunks = split_text_into_chunks(text)
        
        # æ’å…¥æ–‡æœ¬å—åˆ°FTSè¡¨
        for i, chunk in enumerate(chunks):
            cursor.execute("""
                INSERT INTO doc_chunks (chunk_id, content, doc_id, title)
                VALUES (?, ?, ?, ?)
            """, (i, chunk, doc_id, metadata["title"]))
    
    conn.commit()
    print(f"æˆåŠŸå¤„ç† {len(txt_files)} ä¸ªæ–‡ä»¶")

# -----------------------------
# 4ï¸âƒ£ æ··åˆç´¢å¼•ç±»
# -----------------------------
class HybridIndex:
    def __init__(self, conn, embed_model="all-MiniLM-L6-v2"):
        self.conn = conn
        self.embedder = SentenceTransformer(embed_model)
        self.index = None
        self.bm25 = None
        self.chunk_data = []
        self.chunk_texts = []

    def build_indices(self):
        """æ„å»ºFAISSå’ŒBM25ç´¢å¼•"""
        cursor = self.conn.cursor()
        
        cursor.execute("SELECT doc_id, chunk_id, content FROM doc_chunks")
        rows = cursor.fetchall()
        
        embeddings, self.chunk_data, self.chunk_texts = [], [], []
        
        for doc_id, chunk_id, content in rows:
            vec = self.embedder.encode(content, convert_to_numpy=True)
            embeddings.append(vec)
            self.chunk_data.append({
                "doc_id": doc_id,
                "chunk_id": chunk_id,
                "content": content
            })
            self.chunk_texts.append(content)
                   
        if embeddings:
            embeddings = np.array(embeddings).astype("float32")
            dim = embeddings.shape[1]
            self.index = faiss.IndexFlatL2(dim)
            self.index.add(embeddings)
            
            tokenized_docs = [doc.split() for doc in self.chunk_texts]
            self.bm25 = BM25Okapi(tokenized_docs)
            
            print(f"Built indices with {len(embeddings)} chunks")
        else:
            print("Warning: No embeddings created")

    def faiss_search(self, query, k=3):
        if self.index is None:
            return []
        
        q_vec = self.embedder.encode([query], convert_to_numpy=True).astype("float32")
        D, I = self.index.search(q_vec, k)
        
        results = []
        for i, d in zip(I[0], D[0]):
            if i < len(self.chunk_data):
                similarity = float(1 / (1 + d))
                chunk_info = self.chunk_data[i]
                results.append({
                    "doc_id": chunk_info["doc_id"],
                    "chunk_id": chunk_info["chunk_id"],
                    "score": similarity,
                    "content": chunk_info["content"][:200] + "..."
                })
        return results

    def fts5_search(self, query, k=3):
        cursor = self.conn.cursor()
        
        cursor.execute("""
            SELECT doc_id, chunk_id, content 
            FROM doc_chunks 
            WHERE doc_chunks MATCH ? 
            ORDER BY rank
            LIMIT ?
        """, (query, k))
        
        results = []
        for doc_id, chunk_id, content in cursor.fetchall():
            results.append({
                "doc_id": doc_id,
                "chunk_id": chunk_id,
                "score": 0.5,
                "content": content[:200] + "..."
            })
        
        return results

    def bm25_search(self, query, k=3):
        if not self.bm25:
            return []
        
        tokenized_query = query.split()
        scores = self.bm25.get_scores(tokenized_query)
        
        top_indices = np.argsort(scores)[::-1][:k]
        results = []
        
        for idx in top_indices:
            if idx < len(self.chunk_data):
                normalized_score = float(min(scores[idx] / 20, 1.0))
                chunk_info = self.chunk_data[idx]
                results.append({
                    "doc_id": chunk_info["doc_id"],
                    "chunk_id": chunk_info["chunk_id"],
                    "score": normalized_score,
                    "content": chunk_info["content"][:200] + "..."
                })
        
        return results

    def hybrid_search(self, query, k=3, alpha=0.6, use_fts5=False):
        vector_results = self.faiss_search(query, k*2)
        if use_fts5:
            keyword_results = self.fts5_search(query, k*2)
        else:
            keyword_results = self.bm25_search(query, k*2)
        
        def get_key(result):
            return f"{result['doc_id']}_{result['chunk_id']}"
        
        combined = {}
        
        for result in vector_results:
            key = get_key(result)
            combined[key] = {**result, "vector_score": result["score"], "keyword_score": 0.0}
       
        for result in keyword_results:
            key = get_key(result)
            if key in combined:
                combined[key]["keyword_score"] = result["score"]
            else:
                combined[key] = {**result, "vector_score": 0.0, "keyword_score": result["score"]}
        
        final_results = []
        for key, result in combined.items():
            hybrid_score = float(alpha * result["vector_score"] + (1 - alpha) * result["keyword_score"])
            final_results.append({
                "doc_id": result["doc_id"],
                "chunk_id": result["chunk_id"],
                "score": hybrid_score,
                "vector_score": result["vector_score"],
                "keyword_score": result["keyword_score"],
                "content": result["content"]
            })
        
        final_results.sort(key=lambda x: x["score"], reverse=True)
        return final_results[:k]

# -----------------------------
# 5ï¸âƒ£ Jupyterä¸“ç”¨å‡½æ•°
# -----------------------------
def initialize_system():
    """åœ¨Jupyterä¸­åˆå§‹åŒ–æ•´ä¸ªç³»ç»Ÿ"""
    print("Initializing database...")
    conn = init_database()
    print("Populating database from PDF files...")
    populate_database(conn)
    print("Building search indices...")
    hindex = HybridIndex(conn)
    hindex.build_indices()
    print("System initialized successfully!")
    return conn, hindex

def test_queries(hindex, queries=None):
    """æµ‹è¯•æŸ¥è¯¢"""
    if queries is None:
        queries = [
            "machine translation", "transformer", "neural network",
            "deep learning", "artificial intelligence", "natural language processing",
            "computer vision", "reinforcement learning", "graph neural networks",
            "self-supervised learning", "unsupervised learning", "supervised learning",
        ]
    
    for query in queries:
        print(f"\nğŸ” Query: {query}")
        results = hindex.hybrid_search(query, 3)
        for i, result in enumerate(results, 1):
            print(f"  {i}. Score: {result['score']:.3f} | Doc: {result['doc_id']} | Chunk: {result['chunk_id']}")
            print(f"     Content: {result['content']}")

def run_server_in_background():
    """åœ¨åå°è¿è¡ŒæœåŠ¡å™¨ï¼ˆJupyterä¸“ç”¨ï¼‰"""
    import threading
    
    def start_server():
        config = uvicorn.Config(app, host="0.0.0.0", port=8000, log_level="info")
        server = uvicorn.Server(config)
        server.run()
         
    server_thread = threading.Thread(target=start_server, daemon=True)
    server_thread.start()
    print("Server started in background on http://localhost:8000")
    return server_thread
