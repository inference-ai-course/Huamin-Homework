# evaluator.py
import json
from typing import List, Dict

def create_test_questions() -> List[str]:
    """åˆ›å»ºæµ‹è¯•é—®é¢˜"""
    return [
        "What is the main contribution of transformer architectures?",
        "How do authors typically evaluate NLP models?",
        "What datasets are commonly used in machine learning research?",
        "What limitations are often mentioned in AI papers?",
        "How does this approach compare to previous work in the field?"
    ]

def mock_evaluation(test_questions: List[str]) -> List[Dict]:
    """æ¨¡æ‹Ÿæ¨¡å‹è¯„ä¼°"""
    print("ğŸ“Š å¼€å§‹æ¨¡æ‹Ÿæ¨¡å‹è¯„ä¼°...")
    
    results = []
    
    for i, question in enumerate(test_questions, 1):
        print(f"  è¯„ä¼°é—®é¢˜ {i}/{len(test_questions)}: {question[:50]}...")
        
        # æ¨¡æ‹ŸåŸºç¡€æ¨¡å‹å›ç­”
        base_answer = f"Base model response to: {question}. This is a generic response without specific domain knowledge, providing broad overview but lacking technical depth."
        
        # æ¨¡æ‹Ÿå¾®è°ƒæ¨¡å‹å›ç­”
        ft_answer = f"Fine-tuned model response to: {question}. This response demonstrates specialized academic knowledge, using domain-specific terminology and citing relevant research findings from the training data."
        
        results.append({
            "question": question,
            "base_model_answer": base_answer,
            "fine_tuned_answer": ft_answer,
            "improvement": "Fine-tuned model provides more specific, accurate, and technically detailed information"
        })
    
    return results

def save_evaluation_results(results: List[Dict], filename: str = "evaluation_results.json"):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    output_path = f"data/{filename}"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"âœ… å·²ä¿å­˜è¯„ä¼°ç»“æœåˆ° {output_path}")

if __name__ == "__main__":
    questions = create_test_questions()
    results = mock_evaluation(questions)
    save_evaluation_results(results)