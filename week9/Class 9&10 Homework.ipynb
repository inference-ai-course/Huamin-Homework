{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9&10 Assignment: AI Research Assistant Capstone Project\n",
    "\n",
    "\n",
    "## Homework Introduction\n",
    "\n",
    "In this two-week capstone, you will build a **fully integrated AI research assistant** by combining the components developed in earlier weeks. The goal is to create a **real, demo-ready assistant** that can take voice queries, search academic content, summarize findings, and save the results – all in one polished system. This project is **interactive and full-stack**, showcasing skills in ASR, NLP, web APIs, and persistence, making it a strong portfolio piece.\n",
    "\n",
    "* Combine all prior work (ASR, search, summarization, etc.) into one assistant.\n",
    "* Ensure the system is **interactive and persistent** (supports follow-up questions and context).\n",
    "* Deliver a polished demo (voice in/out, UI or CLI) suitable for your career portfolio.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this capstone, you will learn to:\n",
    "\n",
    "* **Integrate multiple AI tools:** Orchestrate ASR (Whisper), LLMs, retrieval, summarization, and TTS into a single assistant.\n",
    "* **Pipeline chaining:** Build a workflow that chains **voice ASR → intent/tool decision → content retrieval → summarization → voice output**.\n",
    "* **Function calling / tool use:** Use LLM function-calling to let the model decide when to invoke tools (e.g. `search_arxiv` vs `summarize`).\n",
    "* **Persistence with Notion:** Automatically sync each session’s conversation and summary into Notion (using a Notion API plugin).\n",
    "* **Session & context management:** Maintain a unique session ID and context history to enable follow-up queries and logging of dialogue.\n",
    "\n",
    "## Project Design\n",
    "\n",
    "You should **reuse and connect** components from previous weeks:\n",
    "\n",
    "* **Whisper ASR (Week 3):** Use the Whisper speech-to-text pipeline to transcribe user voice input. For example, in Python: `model = whisper.load_model(\"base\"); result = model.transcribe(\"audio.wav\")`.\n",
    "* **Retrieval API (Weeks 4–5):** Leverage your vector or hybrid search API (e.g. a semantic search over ArXiv papers) to fetch relevant passages. Implement a function like `search_arxiv(query)` that returns the top-3 relevant documents or excerpts.\n",
    "* **Function Calling Logic (Week 6):** Use function-calling to orchestrate tools. For instance, the LLM’s output might specify calling `search_arxiv`, or `summarize`, or `sync_to_notion`. Wrap each tool call with logging.\n",
    "* **Summarization (Weeks 7–8):** Apply your summarization pipeline or fine-tuned model to condense retrieved content. For example, use Hugging Face’s summarization pipeline (see Transformers docs).\n",
    "* **Notion Sync (Week 1):** Use your Notion integration to save content. Implement a function like `sync_to_notion(session_id, content)` that appends the conversation and summary to a Notion database.\n",
    "\n",
    "**Sample Interaction Flow:** A typical session might work like this:\n",
    "\n",
    "* **User query (Voice → Text):** The user speaks a question. Whisper transcribes it to text.\n",
    "* **Intent & Retrieval:** The assistant (via function-calling) decides to search. It invokes `search_arxiv(query)`, which fetches the top-3 relevant academic passages.\n",
    "* **Summarization:** The passages are passed to `summarize(texts)`, generating a concise answer (e.g. via a Hugging Face pipeline).\n",
    "* **Voice Response (TTS):** The assistant reads the summary aloud using a TTS engine. For example, Python’s `pyttsx3` can speak a string: `engine.say(\"Your summary\")`, `engine.runAndWait()`.\n",
    "* **Notion Sync:** After responding, the full dialogue and the generated summary are synced to Notion with `sync_to_notion(session_id, content)`.\n",
    "\n",
    "**Session Management:** Assign a **unique session ID** to each conversation. Log every query and response (with timestamps) under that session ID. This enables multi-turn dialogues (follow-up questions) and ensures the entire history can be synced to Notion. *Optionally*, maintain chat history or context so that follow-ups build on previous answers.\n",
    "\n",
    "## Starter Code & API Format\n",
    "\n",
    "Your code should clearly define the tool functions and API endpoints. For example:\n",
    "\n",
    "* **Function Signatures:** Create stub functions for your tools, such as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_arxiv(query: str) -> List[str]:\n",
    "      \"\"\"Return relevant document passages for the query.\"\"\"\n",
    "      ...\n",
    "def summarize(texts: List[str]) -> str:\n",
    "      \"\"\"Return a concise summary of the given passages.\"\"\"\n",
    "      ...\n",
    "def sync_to_notion(session_id: str, content: str):\n",
    "      \"\"\"Append the session content and summary to Notion.\"\"\"\n",
    "      ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **LLM Invocation & Logging:** Wrap LLM calls so that you can see inputs/outputs. If using OpenAI’s function-calling, declare these tools so the model can output a JSON like `{\"name\": \"search_arxiv\", \"arguments\": ...}`. When the LLM outputs a function call, execute the corresponding function and feed the result back to the LLM. Log every invocation for debugging.\n",
    "* **API Endpoints (FastAPI):** Design a simple HTTP interface. For example:\n",
    "\n",
    "  * `POST /ask`: Accepts a voice file or text question. Runs the full assistant pipeline and returns the answer (text and/or audio).\n",
    "  * `POST /notion-sync`: (Optional) Manually triggers syncing the current conversation to Notion (or this can be done automatically at session end).\n",
    "  * `GET /status`: Returns the current session ID or a health check status.\n",
    "\n",
    "Make sure each endpoint handler logs activity. For instance, `/ask` should process the request, call Whisper, LLM, summarizer, TTS in order, and return the final response.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "* **Local Development:** Use a Python 3 environment. Install needed libraries: `fastapi`, `uvicorn` (for running the server), `openai-whisper` or `whisper` (for ASR), `transformers` or `llama-cpp-python` (for LLM/summarization), as well as retrieval libraries (`sentence-transformers`, `faiss-cpu`, etc.), and a Notion client (e.g. `notion-client`).\n",
    "* **Running the App:** You can run the assistant locally using FastAPI. For example, start the server with `uvicorn main:app --reload`. You may build a simple frontend (Web or CLI) to record microphone input and play audio output, but a cURL-based or script-based interface is sufficient.\n",
    "* **Notion API:** Create a Notion integration and share the target database/page with it. Obtain the **Notion Secret** and **Database ID**. Store them as environment variables (e.g. `NOTION_TOKEN`, `NOTION_DB_ID`). Use these to authenticate API calls. (You can use the official Notion SDK for Python, or send HTTPS requests to the Notion API.)\n",
    "* **Inference Backend:** Decide on your model setup. You could run a local Llama model via `llama-cpp-python` or use Hugging Face `transformers`. Ensure the model can handle chat and function-calling if needed. Also install any ASR/TTS engines: Whisper (for ASR) and a library like `pyttsx3` or an online TTS (for voice output).\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "* **Code Repository:** A complete, well-organized codebase for the assistant. Structure your code into modules (e.g. `asr.py`, `search.py`, `summarize.py`, `notion.py`, `api.py`). Include comments and docstrings for clarity.\n",
    "* **Demo Video:** A \\~60-second screen recording (you can use any screen-capture tool) demonstrating the assistant in action. The demo should show: speaking a question to the assistant, the assistant retrieving and summarizing a paper, the assistant reading the summary aloud, and the resulting summary appearing in Notion.\n",
    "* **README:** A brief README file that explains how to set up and run your assistant. Include instructions on installing dependencies, setting environment variables (Notion token/DB), and how to start the server or client.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Exploration Tips\n",
    "\n",
    "For extra credit or a more polished project, consider adding:\n",
    "\n",
    "* **Long Conversation Support:** Maintain chat history so the assistant can handle follow-up questions that refer to earlier parts of the conversation.\n",
    "* **Notion Enhancements:** Tag or categorize summaries in Notion (e.g. by topic or source). Include previews or first lines of the content in the synced page. Generate a meaningful title for each entry.\n",
    "* **UI Improvements:** Build a simple frontend (e.g. with Streamlit, Gradio, or a web app) so users can interact without CLI. The UI could record voice input, show the conversation transcript, and embed the Notion page link.\n",
    "* **Additional Features:** Experiment with multi-source search (combining different databases), improve ASR accuracy (handle noise or different accents), or use a better TTS voice for output. Any enhancements that make the assistant more useful or user-friendly are welcome.\n",
    "\n",
    "**Hints:** Make sure to test each component separately first (e.g. whisper transcription, search function, summarizer) before integrating. Keep the code modular so you can easily swap or update parts. Good luck building your AI research assistant!\n",
    "\n",
    "**References:** For example usage of Whisper ASR and summarization pipelines, see the Whisper GitHub and Hugging Face docs. For TTS, Python’s `pyttsx3` is one option. (These are just for guidance on usage; focus on stitching the components together as described.)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
