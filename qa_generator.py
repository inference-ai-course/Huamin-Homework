# qa_generator.py
import json
import re
import time
from pathlib import Path
from typing import List, Dict

# ç›´æ¥å®šä¹‰é…ç½®ï¼Œé¿å…å¯¼å…¥å†²çª
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
DATA_DIR.mkdir(exist_ok=True)

# é…ç½®å‚æ•°
OPENAI_API_KEY = "sk-your-openai-api-key-here"  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
NUM_QA_PAIRS_PER_PAPER = 2
NUM_PAPERS_TO_PROCESS = 3

print("é—®ç­”å¯¹ç”Ÿæˆå™¨")
print(f"æ•°æ®ç›®å½•: {DATA_DIR}")

def mock_gpt4_response(abstract: str, title: str, num_pairs: int) -> str:
    """æ¨¡æ‹ŸGPT-4å“åº”"""
    return f'''
[
    {{
        "question": "What is the main contribution of the paper '{title}'?",
        "answer": "The main contribution is a novel approach to natural language processing that improves performance on benchmark tasks by 25% compared to previous methods."
    }},
    {{
        "question": "What methodology did the authors use in {title}?",
        "answer": "The authors used transformer-based architecture with specialized attention mechanisms and dynamic routing to optimize computational efficiency."
    }},
    {{
        "question": "Does the paper mention using XYZ dataset for evaluation?",
        "answer": "No, the paper does not mention using XYZ dataset. The authors used standard benchmarks including GLUE and SuperGLUE for evaluation."
    }}
]
'''

def generate_qa_pairs(abstract: str, title: str, num_pairs: int = 2) -> List[Dict]:
    """ç”Ÿæˆé—®ç­”å¯¹ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""
    print(f"ä¸ºè®ºæ–‡ '{title}' ç”Ÿæˆ {num_pairs} ä¸ªé—®ç­”å¯¹...")
    
    try:
        # åœ¨å®é™…åº”ç”¨ä¸­å–æ¶ˆæ³¨é‡Šä»¥ä¸‹ä»£ç 
        # import openai
        # openai.api_key = OPENAI_API_KEY
        
        # response = openai.ChatCompletion.create(
        #     model="gpt-4",
        #     messages=[
        #         {"role": "system", "content": "You are a research assistant that creates educational content."},
        #         {"role": "user", "content": f"Create {num_pairs} Q&A pairs for this abstract: {abstract}"}
        #     ],
        #     temperature=0.7,
        #     max_tokens=1500
        # )
        # content = response.choices[0].message.content
        
        # ä½¿ç”¨æ¨¡æ‹Ÿå“åº”
        content = mock_gpt4_response(abstract, title, num_pairs)
        
        # è§£æJSON
        json_match = re.search(r'\[.*\]', content, re.DOTALL)
        if json_match:
            qa_pairs = json.loads(json_match.group())
            return qa_pairs[:num_pairs]  # åªè¿”å›è¯·æ±‚çš„æ•°é‡
        else:
            print("æ— æ³•è§£æå“åº”ä¸­çš„JSONï¼Œä½¿ç”¨é»˜è®¤é—®ç­”å¯¹")
            return create_default_qa_pairs(title, num_pairs)
            
    except Exception as e:
        print(f"ç”Ÿæˆé—®ç­”å¯¹æ—¶å‡ºé”™: {e}")
        return create_default_qa_pairs(title, num_pairs)

def create_default_qa_pairs(title: str, num_pairs: int) -> List[Dict]:
    """åˆ›å»ºé»˜è®¤çš„é—®ç­”å¯¹"""
    default_pairs = [
        {
            "question": f"What is the primary focus of the paper '{title}'?",
            "answer": f"The paper '{title}' focuses on developing innovative machine learning techniques to address computational challenges in natural language processing."
        },
        {
            "question": f"What are the key findings reported in {title}?",
            "answer": f"The key findings include significant improvements in model efficiency and performance on standard benchmarks, with a 30% reduction in computational requirements."
        },
        {
            "question": f"Did the authors of {title} compare their approach with existing methods?",
            "answer": f"Yes, the authors conducted comprehensive comparisons with state-of-the-art methods and demonstrated superior performance across multiple evaluation metrics."
        }
    ]
    return default_pairs[:num_pairs]

def read_paper_texts() -> List[Dict]:
    """è¯»å–ä¹‹å‰æå–çš„è®ºæ–‡æ–‡æœ¬"""
    text_dir = BASE_DIR / "arxiv_texts"
    papers = []
    
    if not text_dir.exists():
        print(f"ç›®å½• {text_dir} ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        return create_sample_papers()
    
    text_files = list(text_dir.glob("*.txt"))
    if not text_files:
        print("æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®")
        return create_sample_papers()
    
    for text_file in text_files:
        try:
            with open(text_file, "r", encoding="utf-8") as f:
                content = f.read()
            
            # ç®€å•è§£ææ–‡æœ¬æ–‡ä»¶
            lines = content.split('\n')
            title = lines[0].replace("Title: ", "") if lines[0].startswith("Title: ") else text_file.stem
            abstract = lines[1].replace("Abstract: ", "") if len(lines) > 1 and lines[1].startswith("Abstract: ") else "No abstract available"
            
            papers.append({
                "title": title,
                "abstract": abstract,
                "source_file": text_file.name
            })
            
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {text_file} æ—¶å‡ºé”™: {e}")
    
    return papers

def create_sample_papers() -> List[Dict]:
    """åˆ›å»ºç¤ºä¾‹è®ºæ–‡æ•°æ®"""
    return [
        {
            "title": "transformer_research",
            "abstract": "This paper introduces a novel transformer architecture that improves computational efficiency while maintaining performance. Our approach utilizes sparse attention mechanisms.",
            "source_file": "sample_1.txt"
        },
        {
            "title": "machine_learning_survey", 
            "abstract": "We present a comprehensive survey of machine learning techniques, focusing on deep learning architectures and their applications in various domains.",
            "source_file": "sample_2.txt"
        },
        {
            "title": "ai_advances",
            "abstract": "This research paper presents advancements in AI methodologies, demonstrating improved performance across multiple benchmarks while reducing computational requirements.",
            "source_file": "sample_3.txt"
        }
    ]

def generate_all_qa_papers() -> List[Dict]:
    """ä¸ºæ‰€æœ‰è®ºæ–‡ç”Ÿæˆé—®ç­”å¯¹"""
    print("è¯»å–è®ºæ–‡æ•°æ®...")
    papers = read_paper_texts()
    
    if not papers:
        print("æ²¡æœ‰è®ºæ–‡æ•°æ®å¯ç”¨")
        return []
    
    print(f"æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    all_qa_pairs = []
    
    for i, paper in enumerate(papers[:NUM_PAPERS_TO_PROCESS]):
        print(f"\nå¤„ç†è®ºæ–‡ {i+1}/{min(NUM_PAPERS_TO_PROCESS, len(papers))}: {paper['title']}")
        
        qa_pairs = generate_qa_pairs(paper['abstract'], paper['title'], NUM_QA_PAIRS_PER_PAPER)
        
        if qa_pairs:
            for qa in qa_pairs:
                qa['paper_title'] = paper['title']
                qa['paper_abstract'] = paper['abstract'][:100] + "..."
            
            all_qa_pairs.extend(qa_pairs)
            print(f"âœ“ ç”Ÿæˆ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
        else:
            print("âœ— ç”Ÿæˆé—®ç­”å¯¹å¤±è´¥")
        
        time.sleep(0.5)  # æ¨¡æ‹Ÿå»¶è¿Ÿ
    
    return all_qa_pairs

def save_qa_pairs(qa_pairs: List[Dict], filename: str = "synthetic_qa_data.json"):
    """ä¿å­˜é—®ç­”å¯¹"""
    output_path = DATA_DIR / filename
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(qa_pairs, f, indent=2, ensure_ascii=False)
    print(f"âœ“ å·²ä¿å­˜ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹åˆ° {output_path}")

def display_qa_pairs(qa_pairs: List[Dict]):
    """æ˜¾ç¤ºç”Ÿæˆçš„é—®ç­”å¯¹"""
    print("\n" + "=" * 80)
    print("ç”Ÿæˆçš„é—®ç­”å¯¹é¢„è§ˆ:")
    print("=" * 80)
    
    for i, qa in enumerate(qa_pairs, 1):
        print(f"\n{i}. è®ºæ–‡: {qa.get('paper_title', 'Unknown')}")
        print(f"   é—®é¢˜: {qa['question']}")
        print(f"   ç­”æ¡ˆ: {qa['answer'][:100]}...")
        if i >= 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   ... è¿˜æœ‰ {len(qa_pairs) - 5} ä¸ªé—®ç­”å¯¹")
            break

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("é—®ç­”å¯¹ç”Ÿæˆæ¨¡å—")
    print("=" * 60)
    
    # ç”Ÿæˆé—®ç­”å¯¹
    qa_pairs = generate_all_qa_papers()
    
    if qa_pairs:
        # ä¿å­˜ç»“æœ
        save_qa_pairs(qa_pairs)
        
        # æ˜¾ç¤ºé¢„è§ˆ
        display_qa_pairs(qa_pairs)
        
        print(f"\nâœ… æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
        print(f"ğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {DATA_DIR / 'synthetic_qa_data.json'}")
    else:
        print("âŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•é—®ç­”å¯¹")

if __name__ == "__main__":
    main()