# main.py
import json
import sys
from pathlib import Path
import subprocess

# é…ç½®å‚æ•°
NUM_PAPERS_TO_PROCESS = 10    # å¤„ç†å¤šå°‘ç¯‡è®ºæ–‡ï¼ˆä½ æœ‰50ç¯‡ï¼Œè¿™é‡Œå¤„ç†10ç¯‡ï¼‰
NUM_QA_PAIRS_PER_PAPER = 3    # æ¯ç¯‡è®ºæ–‡ç”Ÿæˆå‡ ä¸ªé—®ç­”å¯¹

def run_w4_homework_if_needed():
    """å¦‚æœéœ€è¦ï¼Œè¿è¡Œw4-homework.pyæ¥è·å–æ•°æ®"""
    pdf_dir = Path("arxiv_pdfs")
    text_dir = Path("arxiv_texts")
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
    has_pdfs = pdf_dir.exists() and any(pdf_dir.glob("*.pdf"))
    has_texts = text_dir.exists() and any(text_dir.glob("*.txt"))
    
    if has_pdfs and has_texts:
        print("âœ… å‘ç°å·²æœ‰çš„PDFå’Œæ–‡æœ¬æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨")
        pdf_files = list(pdf_dir.glob("*.pdf"))
        text_files = list(text_dir.glob("*.txt"))
        print(f"   PDFæ–‡ä»¶: {len(pdf_files)} ä¸ª")
        print(f"   æ–‡æœ¬æ–‡ä»¶: {len(text_files)} ä¸ª")
        return True
    
    print("ğŸ“¥ æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„æ•°æ®ï¼Œéœ€è¦è¿è¡Œw4-homework.py...")
    try:
        # è¿è¡Œä½ çš„w4-homework.py
        result = subprocess.run(
            [sys.executable, "w4-Homework.py"],
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            print("âœ… w4-homework.py è¿è¡ŒæˆåŠŸ")
            print(result.stdout[-500:])  # æ˜¾ç¤ºæœ€å500å­—ç¬¦çš„è¾“å‡º
            return True
        else:
            print("âŒ w4-homework.py è¿è¡Œå¤±è´¥")
            print("é”™è¯¯è¾“å‡º:", result.stderr)
            return False
            
    except subprocess.TimeoutExpired:
        print("âŒ w4-homework.py è¿è¡Œè¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ è¿è¡Œw4-homework.pyæ—¶å‡ºé”™: {e}")
        return False

def get_existing_papers():
    """ä»å·²æœ‰æ–‡æœ¬æ–‡ä»¶ä¸­è·å–è®ºæ–‡æ•°æ®"""
    text_dir = Path("arxiv_texts")
    papers = []
    
    if not text_dir.exists():
        print("âŒ arxiv_textsç›®å½•ä¸å­˜åœ¨")
        return []
    
    text_files = list(text_dir.glob("*.txt"))
    if not text_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬æ–‡ä»¶")
        return []
    
    print(f"ğŸ“– ä» {len(text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶ä¸­è¯»å–æ•°æ®...")
    
    for i, text_file in enumerate(text_files[:NUM_PAPERS_TO_PROCESS]):
        try:
            with open(text_file, "r", encoding="utf-8") as f:
                content = f.read()
            
            # ç®€å•è§£ææ–‡æœ¬æ–‡ä»¶å†…å®¹
            lines = content.split('\n')
            title = "æœªçŸ¥æ ‡é¢˜"
            abstract = "æ— æ‘˜è¦"
            full_text = content
            
            # å°è¯•æå–æ ‡é¢˜å’Œæ‘˜è¦
            for line in lines:
                if line.startswith("Title:"):
                    title = line.replace("Title:", "").strip()
                elif line.startswith("Abstract:"):
                    abstract = line.replace("Abstract:", "").strip()
                elif line.startswith("Text:"):
                    full_text = line.replace("Text:", "").strip()
            
            papers.append({
                "title": title,
                "abstract": abstract,
                "full_text": full_text[:1000],  # é™åˆ¶é•¿åº¦
                "source_file": text_file.name
            })
            
            print(f"   âœ… å·²åŠ è½½: {title}")
            
        except Exception as e:
            print(f"   âŒ è¯»å– {text_file} å¤±è´¥: {e}")
    
    return papers

def run_qa_generation(papers):
    """è¿è¡Œé—®ç­”å¯¹ç”Ÿæˆ"""
    print("\nğŸ¤– ç”Ÿæˆé—®ç­”å¯¹...")
    try:
        # åŠ¨æ€å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
        from qa_generator import generate_all_qa_papers, save_qa_pairs
        
        qa_pairs = generate_all_qa_papers()
        if qa_pairs:
            save_qa_pairs(qa_pairs)
            print(f"âœ… æˆåŠŸç”Ÿæˆ {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
            
            # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
            print("\nğŸ“‹ é—®ç­”å¯¹ç¤ºä¾‹:")
            for i, qa in enumerate(qa_pairs[:3]):
                print(f"   {i+1}. Q: {qa['question'][:50]}...")
                print(f"      A: {qa['answer'][:50]}...")
                print()
            
            return qa_pairs
        else:
            print("âŒ æ²¡æœ‰ç”Ÿæˆé—®ç­”å¯¹")
            return None
            
    except Exception as e:
        print(f"âŒ é—®ç­”ç”Ÿæˆé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def run_data_formatting(qa_pairs):
    """è¿è¡Œæ•°æ®æ ¼å¼åŒ–"""
    print("\nğŸ“ æ ¼å¼åŒ–æ•°æ®...")
    try:
        from data_formatter import convert_to_jsonl_format
        
        formatted_data = convert_to_jsonl_format(qa_pairs)
        print(f"âœ… æˆåŠŸæ ¼å¼åŒ– {len(formatted_data)} æ¡æ•°æ®")
        return formatted_data
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ ¼å¼åŒ–é”™è¯¯: {e}")
        return None
    
def run_model_training():
    """è¿è¡Œæ¨¡å‹è®­ç»ƒï¼ˆçœŸå®GPUè®­ç»ƒï¼‰"""
    print("\nâš™ï¸ è®­ç»ƒæ¨¡å‹...")
    try:
        from model_trainer import real_fine_tuning  # â† å¯¼å…¥çœŸå®è®­ç»ƒå‡½æ•°
        
        model_info = real_fine_tuning()  # â† è°ƒç”¨çœŸå®è®­ç»ƒå‡½æ•°
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return model_info
    except Exception as e:
        print(f"âŒ æ¨¡å‹è®­ç»ƒé”™è¯¯: {e}")
        return {"status": "error", "message": str(e)}

def run_evaluation():
    """è¿è¡Œæ¨¡å‹è¯„ä¼°"""
    print("\nğŸ“Š è¯„ä¼°æ¨¡å‹...")
    try:
        from evaluator import create_test_questions, mock_evaluation, save_evaluation_results
        
        test_questions = create_test_questions()
        evaluation_results = mock_evaluation(test_questions)
        save_evaluation_results(evaluation_results)
        print(f"âœ… å®Œæˆ {len(evaluation_results)} ä¸ªæµ‹è¯•é—®é¢˜çš„è¯„ä¼°")
        return evaluation_results
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¯„ä¼°é”™è¯¯: {e}")
        return []

def generate_summary_report(papers, qa_pairs, model_info, evaluation_results):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    report = {
        "total_papers_available": len(papers),
        "total_qa_pairs_generated": len(qa_pairs) if qa_pairs else 0,
        "model_training_info": model_info,
        "evaluation_summary": {
            "questions_tested": len(evaluation_results),
            "improvements_noted": len([r for r in evaluation_results if "improvement" in r])
        },
        "config": {
            "papers_processed": NUM_PAPERS_TO_PROCESS,
            "qa_per_paper": NUM_QA_PAIRS_PER_PAPER
        }
    }
    
    with open("data/summary_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print("ğŸ“Š æ€»ç»“æŠ¥å‘Šå·²ç”Ÿæˆ: data/summary_report.json")
    
    # æ‰“å°ç®€è¦ä¿¡æ¯
    print(f"\nğŸ¯ é¡¹ç›®æ€»ç»“:")
    print(f"   â€¢ å¯ç”¨è®ºæ–‡: {len(papers)} ç¯‡")
    print(f"   â€¢ ç”Ÿæˆé—®ç­”: {len(qa_pairs) if qa_pairs else 0} å¯¹")
    print(f"   â€¢ æµ‹è¯•é—®é¢˜: {len(evaluation_results)} ä¸ª")
    print(f"   â€¢ æ¨¡å‹çŠ¶æ€: {model_info.get('status', 'N/A') if model_info else 'N/A'}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ¤– Week 7 Homework: å­¦æœ¯é—®ç­”ç³»ç»Ÿ")
    print("=" * 60)
    
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    required_dirs = ["arxiv_pdfs", "arxiv_texts", "data"]
    for dir_name in required_dirs:
        dir_path = Path(dir_name)
        dir_path.mkdir(exist_ok=True)
        print(f"ğŸ“ ç¡®ä¿ç›®å½•å­˜åœ¨: {dir_path}")
    
    # 1. æ•°æ®å‡†å¤‡ - è¿è¡Œæˆ–ä½¿ç”¨ç°æœ‰çš„w4-homeworkæ•°æ®
    print("\n1. ğŸ“¥ æ•°æ®å‡†å¤‡é˜¶æ®µ")
    if not run_w4_homework_if_needed():
        print("âŒ æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # 2. è·å–è®ºæ–‡æ•°æ®
    print("\n2. ğŸ“„ è¯»å–è®ºæ–‡æ•°æ®")
    papers = get_existing_papers()
    if not papers:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„è®ºæ–‡æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    # 3. ç”Ÿæˆé—®ç­”å¯¹
    qa_pairs = run_qa_generation(papers)
    if not qa_pairs:
        print("âŒ é—®ç­”å¯¹ç”Ÿæˆå¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        return
    
    # 4. æ•°æ®æ ¼å¼åŒ–
    formatted_data = run_data_formatting(qa_pairs)
    if not formatted_data:
        print("âš ï¸  æ•°æ®æ ¼å¼åŒ–å¤±è´¥ï¼Œç»§ç»­åç»­æ­¥éª¤")
    
    # 5. æ¨¡å‹è®­ç»ƒï¼ˆæ¨¡æ‹Ÿï¼‰
    model_info = run_model_training()
    
    # 6. æ¨¡å‹è¯„ä¼°
    evaluation_results = run_evaluation()
    
    # 7. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    generate_summary_report(papers, qa_pairs, model_info, evaluation_results)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æ‰€æœ‰æ­¥éª¤å®Œæˆï¼")
    print("=" * 60)
    
    # æ˜¾ç¤ºä¸‹ä¸€æ­¥å»ºè®®
    print("\nğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. æŸ¥çœ‹ data/synthetic_qa_data.json - ç”Ÿæˆçš„é—®ç­”å¯¹")
    print("2. æŸ¥çœ‹ data/summary_report.json - é¡¹ç›®æ€»ç»“æŠ¥å‘Š")
    print("3. æŸ¥çœ‹ arxiv_texts/ - æå–çš„è®ºæ–‡æ–‡æœ¬")
    print("4. å¦‚æœéœ€è¦çœŸå®è®­ç»ƒï¼Œè®¾ç½®OPENAI_API_KEYå¹¶ä¿®æ”¹ä»£ç ")

if __name__ == "__main__":
    main()