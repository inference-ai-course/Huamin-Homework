{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb63cab-19a8-4ee4-a3d0-06376999a042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "导入完成！配置就绪。\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: 导入和配置\n",
    "import sqlite3\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "from contextlib import asynccontextmanager\n",
    "import nest_asyncio\n",
    "import requests\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# 修复事件循环问题\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# -----------------------------\n",
    "# 全局配置\n",
    "# -----------------------------\n",
    "PDF_DIR = Path(\"arxiv_pdfs\")\n",
    "TXT_DIR = Path(\"arxiv_texts\")  # 使用week4作业已经生成的文本文件\n",
    "DB_PATH = \"hybrid_search.db\"\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# 确保目录存在\n",
    "PDF_DIR.mkdir(exist_ok=True)\n",
    "TXT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"导入完成！配置就绪。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fdcc57-769c-4706-854b-171a03eba527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本处理函数定义完成！\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: 文本处理函数（不使用fitz）\n",
    "def extract_metadata_from_filename(filename):\n",
    "    \"\"\"从文件名中提取元数据\"\"\"\n",
    "    # 移除文件扩展名\n",
    "    name_without_ext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # 根据实际文件名格式进行调整\n",
    "    \n",
    "    # 示例：简单的分割方式\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    metadata = {\n",
    "        \"title\": parts[0] if len(parts) > 0 else \"未知标题\",\n",
    "        \"authors\": parts[1] if len(parts) > 1 else \"未知作者\",\n",
    "        \"year\": int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 2023,\n",
    "        \"abstract\": \"\",  # 默认空摘要\n",
    "        \"keywords\": \"\"   # 默认空关键词\n",
    "    }\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def load_text_from_file(txt_file: Path) -> str:\n",
    "    \"\"\"从文本文件加载内容\"\"\"\n",
    "    try:\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(doc_id: str, text: str, chunk_size=500, overlap=100) -> list:\n",
    "    \"\"\"文本分块\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    chunk_id = 0\n",
    "    while i < len(words):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        chunks.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": chunk_text\n",
    "        })\n",
    "        i += chunk_size - overlap\n",
    "        chunk_id += 1\n",
    "    return chunks\n",
    "\n",
    "print(\"文本处理函数定义完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec03bd-a464-4a80-aeed-65771d4f125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: 数据库初始化\n",
    "def init_database():\n",
    "    \"\"\"初始化数据库，创建必要的表\"\"\"\n",
    "    conn = sqlite3.connect('document_search.db')\n",
    "    \n",
    "    # 创建文档表 - 为与插入语句匹配的结构\n",
    "    conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        doc_id TEXT PRIMARY KEY,\n",
    "        title TEXT NOT NULL,\n",
    "        authors TEXT,\n",
    "        year INTEGER,\n",
    "        abstract TEXT,\n",
    "        keywords TEXT,\n",
    "        file_path TEXT NOT NULL,\n",
    "        upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # 创建FTS5虚拟表用于全文搜索\n",
    "    conn.execute(\"\"\"\n",
    "    CREATE VIRTUAL TABLE IF NOT EXISTS doc_chunks USING fts5(\n",
    "        chunk_id UNINDEXED,\n",
    "        content,\n",
    "        doc_id UNINDEXED,\n",
    "        title\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"数据库初始化完成！\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19f5047a-26b4-4609-a375-6ffad205cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 数据库填充函数\n",
    "def split_text_into_chunks(text, chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    将文本分割成重叠的块\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        if end > text_length:\n",
    "            end = text_length\n",
    "        \n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # 移动起始位置，考虑重叠\n",
    "        start += chunk_size - overlap\n",
    "        \n",
    "        # 确保不会无限循环\n",
    "        if start >= text_length:\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "def populate_database(conn):\n",
    "    \"\"\"从文本文件填充数据库\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # 获取所有文本文件\n",
    "    txt_files = glob.glob(os.path.join(TXT_DIR, \"*.txt\"))\n",
    "    print(f\"找到 {len(txt_files)} 个文本文件\")\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        # 从文件名提取文档ID（不含扩展名）\n",
    "        doc_id = os.path.splitext(os.path.basename(txt_file))[0]\n",
    "        \n",
    "        # 从文件名提取元数据\n",
    "        metadata = extract_metadata_from_filename(doc_id)\n",
    "        \n",
    "        # 插入文档元数据 - 添加 file_path 字段\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO documents (doc_id, title, authors, year, abstract, keywords, file_path)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (doc_id, metadata[\"title\"], metadata[\"authors\"], \n",
    "              metadata[\"year\"], metadata[\"abstract\"], \"\", txt_file))  # 添加 txt_file 作为 file_path\n",
    "        \n",
    "        # 加载文本内容\n",
    "        text = load_text_from_file(txt_file)\n",
    "        \n",
    "        # 分割文本为块\n",
    "        chunks = split_text_into_chunks(text)\n",
    "        \n",
    "        # 插入文本块到FTS表\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO doc_chunks (chunk_id, content, doc_id, title)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "            \"\"\", (i, chunk, doc_id, metadata[\"title\"]))\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"成功处理 {len(txt_files)} 个文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fb7593c-8735-4bbd-911e-dd0d7aaa62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4️⃣ 混合索引类\n",
    "# -----------------------------\n",
    "class HybridIndex:\n",
    "    def __init__(self, conn, embed_model=\"all-MiniLM-L6-v2\"):\n",
    "        self.conn = conn\n",
    "        self.embedder = SentenceTransformer(embed_model)\n",
    "        self.index = None\n",
    "        self.bm25 = None\n",
    "        self.chunk_data = []\n",
    "        self.chunk_texts = []\n",
    "\n",
    "    def build_indices(self):\n",
    "        \"\"\"构建FAISS和BM25索引\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"SELECT doc_id, chunk_id, content FROM doc_chunks\")\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        embeddings, self.chunk_data, self.chunk_texts = [], [], []\n",
    "        \n",
    "        for doc_id, chunk_id, content in rows:\n",
    "            vec = self.embedder.encode(content, convert_to_numpy=True)\n",
    "            embeddings.append(vec)\n",
    "            self.chunk_data.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"content\": content\n",
    "            })\n",
    "            self.chunk_texts.append(content)\n",
    "        \n",
    "        if embeddings:\n",
    "            embeddings = np.array(embeddings).astype(\"float32\")\n",
    "            dim = embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatL2(dim)\n",
    "            self.index.add(embeddings)\n",
    "            \n",
    "            tokenized_docs = [doc.split() for doc in self.chunk_texts]\n",
    "            self.bm25 = BM25Okapi(tokenized_docs)\n",
    "            \n",
    "            print(f\"Built indices with {len(embeddings)} chunks\")\n",
    "        else:\n",
    "            print(\"Warning: No embeddings created\")\n",
    "\n",
    "    def faiss_search(self, query, k=3):\n",
    "        if self.index is None:\n",
    "            return []\n",
    "        \n",
    "        q_vec = self.embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "        D, I = self.index.search(q_vec, k)\n",
    "        \n",
    "        results = []\n",
    "        for i, d in zip(I[0], D[0]):\n",
    "            if i < len(self.chunk_data):\n",
    "                similarity = float(1 / (1 + d))\n",
    "                chunk_info = self.chunk_data[i]\n",
    "                results.append({\n",
    "                    \"doc_id\": chunk_info[\"doc_id\"],\n",
    "                    \"chunk_id\": chunk_info[\"chunk_id\"],\n",
    "                    \"score\": similarity,\n",
    "                    \"content\": chunk_info[\"content\"][:200] + \"...\"\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def fts5_search(self, query, k=3):\n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT doc_id, chunk_id, content \n",
    "            FROM doc_chunks \n",
    "            WHERE doc_chunks MATCH ? \n",
    "            ORDER BY rank\n",
    "            LIMIT ?\n",
    "        \"\"\", (query, k))\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, chunk_id, content in cursor.fetchall():\n",
    "            results.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"score\": 0.5,\n",
    "                \"content\": content[:200] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def bm25_search(self, query, k=3):\n",
    "        if not self.bm25:\n",
    "            return []\n",
    "        \n",
    "        tokenized_query = query.split()\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        top_indices = np.argsort(scores)[::-1][:k]\n",
    "        results = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if idx < len(self.chunk_data):\n",
    "                normalized_score = float(min(scores[idx] / 20, 1.0))\n",
    "                chunk_info = self.chunk_data[idx]\n",
    "                results.append({\n",
    "                    \"doc_id\": chunk_info[\"doc_id\"],\n",
    "                    \"chunk_id\": chunk_info[\"chunk_id\"],\n",
    "                    \"score\": normalized_score,\n",
    "                    \"content\": chunk_info[\"content\"][:200] + \"...\"\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def hybrid_search(self, query, k=3, alpha=0.6, use_fts5=False):\n",
    "        vector_results = self.faiss_search(query, k*2)\n",
    "        if use_fts5:\n",
    "            keyword_results = self.fts5_search(query, k*2)\n",
    "        else:\n",
    "            keyword_results = self.bm25_search(query, k*2)\n",
    "        \n",
    "        def get_key(result):\n",
    "            return f\"{result['doc_id']}_{result['chunk_id']}\"\n",
    "        \n",
    "        combined = {}\n",
    "        \n",
    "        for result in vector_results:\n",
    "            key = get_key(result)\n",
    "            combined[key] = {**result, \"vector_score\": result[\"score\"], \"keyword_score\": 0.0}\n",
    "        \n",
    "        for result in keyword_results:\n",
    "            key = get_key(result)\n",
    "            if key in combined:\n",
    "                combined[key][\"keyword_score\"] = result[\"score\"]\n",
    "            else:\n",
    "                combined[key] = {**result, \"vector_score\": 0.0, \"keyword_score\": result[\"score\"]}\n",
    "        \n",
    "        final_results = []\n",
    "        for key, result in combined.items():\n",
    "            hybrid_score = float(alpha * result[\"vector_score\"] + (1 - alpha) * result[\"keyword_score\"])\n",
    "            final_results.append({\n",
    "                \"doc_id\": result[\"doc_id\"],\n",
    "                \"chunk_id\": result[\"chunk_id\"],\n",
    "                \"score\": hybrid_score,\n",
    "                \"vector_score\": result[\"vector_score\"],\n",
    "                \"keyword_score\": result[\"keyword_score\"],\n",
    "                \"content\": result[\"content\"]\n",
    "            })\n",
    "        \n",
    "        final_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return final_results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d6f06b3-39ca-4565-8799-5253cf875c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5️⃣ Jupyter专用函数\n",
    "# -----------------------------\n",
    "def initialize_system():\n",
    "    \"\"\"在Jupyter中初始化整个系统\"\"\"\n",
    "    print(\"Initializing database...\")\n",
    "    conn = init_database()\n",
    "    print(\"Populating database from PDF files...\")\n",
    "    populate_database(conn)\n",
    "    print(\"Building search indices...\")\n",
    "    hindex = HybridIndex(conn)\n",
    "    hindex.build_indices()\n",
    "    print(\"System initialized successfully!\")\n",
    "    return conn, hindex\n",
    "\n",
    "def test_queries(hindex, queries=None):\n",
    "    \"\"\"测试查询\"\"\"\n",
    "    if queries is None:\n",
    "        queries = [\n",
    "            \"machine translation\", \"transformer\", \"neural network\",\n",
    "            \"deep learning\", \"artificial intelligence\", \"natural language processing\",\n",
    "            \"computer vision\", \"reinforcement learning\", \"graph neural networks\",\n",
    "            \"self-supervised learning\", \"unsupervised learning\", \"supervised learning\",\n",
    "        ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\n🔍 Query: {query}\")\n",
    "        results = hindex.hybrid_search(query, 3)\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {i}. Score: {result['score']:.3f} | Doc: {result['doc_id']} | Chunk: {result['chunk_id']}\")\n",
    "            print(f\"     Content: {result['content']}\")\n",
    "\n",
    "def run_server_in_background():\n",
    "    \"\"\"在后台运行服务器（Jupyter专用）\"\"\"\n",
    "    import threading\n",
    "    \n",
    "    def start_server():\n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "        server = uvicorn.Server(config)\n",
    "        server.run()\n",
    "    \n",
    "    server_thread = threading.Thread(target=start_server, daemon=True)\n",
    "    server_thread.start()\n",
    "    print(\"Server started in background on http://localhost:8000\")\n",
    "    return server_thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db7b5f28-e85c-44a1-b73a-675ccf45ea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing database...\n",
      "数据库初始化完成！\n",
      "Populating database from PDF files...\n",
      "找到 50 个文本文件\n",
      "成功处理 50 个文件\n",
      "Building search indices...\n",
      "Built indices with 7709 chunks\n",
      "System initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: 执行初始化\n",
    "conn, hindex = initialize_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df225e47-ee5a-410a-a8d4-c2b7362f8901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query: machine translation\n",
      "  1. Score: 0.568 | Doc: Estimating_Machine_Translation_Difficulty | Chunk: 78\n",
      "     Content: t using the quality of the translations it produces. While we acknowledge that translation difficulty should ideally be an in- trinsic property of the source – independent of any specific translation ...\n",
      "  2. Score: 0.546 | Doc: Estimating_Machine_Translation_Difficulty | Chunk: 17\n",
      "     Content: ion, meaning that the same source text might be more difficult to translate into one lan- guage than into another. Moreover, translation diffi- culty might not be uniform across translators. It can va...\n",
      "  3. Score: 0.388 | Doc: Estimating_Machine_Translation_Difficulty | Chunk: 0\n",
      "     Content: Estimating Machine Translation Difficulty Lorenzo Proietti1,∗ Stefano Perrella1,∗ Vilém Zouhar2,∗ Roberto Navigli1 Tom Kocmi3 1Sapienza NLP Group, Sapienza University of Rome 2ETH Zurich 3 Cohere {lpr...\n",
      "\n",
      "🔍 Query: transformer\n",
      "  1. Score: 0.316 | Doc: Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of\n",
      "__Gloss_A | Chunk: 59\n",
      "     Content: discuss the theoretical underpinnings of our approach in chapter 2, with the transformer serving as the primary focal point. Discuss the works that have already Chapter 1. Introduction 7 been done on ...\n",
      "  2. Score: 0.313 | Doc: eDIF__A_European_Deep_Inference_Fabric_for_Remote_Interpretability_of\n",
      "__LLM | Chunk: 91\n",
      "     Content: om/TransformerLensOrg/ TransformerLens [2] C. Tang, B. Lake, and M. Jazayeri, “An explainable transformer circuit for compositional generalization,” arXiv preprint arXiv:2502.15801, Feb. 2025. [Online...\n",
      "  3. Score: 0.310 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 25\n",
      "     Content: th content-based and location- based—addressing the challenges of training a transformer from scratch with fewer than 100,000 data points. By applying data augmentations exclusively during training, t...\n",
      "\n",
      "🔍 Query: neural network\n",
      "  1. Score: 0.495 | Doc: Computational_Economics_in_Large_Language_Models__Exploring_Model\n",
      "__Behavior_and | Chunk: 36\n",
      "     Content: ernal components of a neural network itself. This perspective is supported by the information bottleneck principle, which posits that any learning system should optimally trade off between compressing...\n",
      "  2. Score: 0.490 | Doc: Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of\n",
      "__Gloss_A | Chunk: 80\n",
      "     Content: morphism test[20]. Regardless of the motive, a GNN’s distinguishing fea- ture is that it employs message passing between neighbourhood nodes and update this information using neural network similar to...\n",
      "  3. Score: 0.304 | Doc: Computational_Economics_in_Large_Language_Models__Exploring_Model\n",
      "__Behavior_and | Chunk: 43\n",
      "     Content: icient activations. head is considered an agent. For the feed-forward network (FFN), each neuron (or a group of neurons) can be modeled as an agent. These agents are responsible for processing informa...\n",
      "\n",
      "🔍 Query: deep learning\n",
      "  1. Score: 0.314 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 4\n",
      "     Content: ter recognition proves to be a complex endeavor since the shapes of letters are influenced by numerous factors, including the writer’s style, the type of pen used, the surface material, and the direct...\n",
      "  2. Score: 0.307 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 28\n",
      "     Content: Neural Networks (CNNs) can be effective. For instance, a Bidirectional LSTM possesses the advantage of capturing more sequential information compared to its unidirectional counterpart [GS05], making i...\n",
      "  3. Score: 0.296 | Doc: Understanding_Textual_Emotion_Through_Emoji_Prediction | Chunk: 13\n",
      "     Content:  2.1. Design Choices and Implementation To conduct this project and explore various approaches, four models are created using different architectures: BERT, a feedforward neural network, a transformer...\n",
      "\n",
      "🔍 Query: artificial intelligence\n",
      "  1. Score: 0.309 | Doc: Computational_Economics_in_Large_Language_Models__Exploring_Model\n",
      "__Behavior_and | Chunk: 35\n",
      "     Content: verse engineering” of game theory; it focuses on designing the rules of a game to incentivize self-interested agents to behave in a way that achieves a desirable system-wide outcome [46]. This framewo...\n",
      "  2. Score: 0.297 | Doc: Making_Qwen3_Think_in_Korean_with_Reinforcement_Learning | Chunk: 80\n",
      "     Content: ng an AI “think” in the user’s language opens the door for more inclusive AI systems globally. Rather than treating English as the default language of intelligence, our work suggests that with focused...\n",
      "  3. Score: 0.295 | Doc: mSCoRe__a_$M$ultilingual_and_Scalable_Benchmark_for_$S$kill-based\n",
      "__$Co$mmonsens | Chunk: 215\n",
      "     Content:  AI's ability to utilize commonsense reasoning to arrive at the correct answer. The goal is to test and enhance the AI's understanding of cultural norms and behaviors in a specific setting. Provide th...\n",
      "\n",
      "🔍 Query: natural language processing\n",
      "  1. Score: 0.341 | Doc: LaajMeter__A_Framework_for_LaaJ_Evaluation | Chunk: 4\n",
      "     Content: ities of Natural Language Processing (NLP), making a wide range of tasks such as translation, summarization, question answering, and code generation not only feasible but scalable and accessible. Thei...\n",
      "  2. Score: 0.320 | Doc: Improving_Generative_Cross-lingual_Aspect-Based_Sentiment_Analysis_with\n",
      "__Constr | Chunk: 175\n",
      "     Content: logy.org/N18-1053 2. Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., Do, Q.V., Xu, Y., Fung, P.: A multitask, multilin- gual, multimodal evaluat...\n",
      "  3. Score: 0.318 | Doc: Large_Language_Models_for_Summarizing_Czech_Historical_Documents_and\n",
      "__Beyond | Chunk: 61\n",
      "     Content:  J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow- icz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoes...\n",
      "\n",
      "🔍 Query: computer vision\n",
      "  1. Score: 0.296 | Doc: CorrectNav__Self-Correction_Flywheel_Empowers_Vision-Language-Action\n",
      "__Navigatio | Chunk: 43\n",
      "     Content: tion keyframes {K1, K2, K3}. We then lever- age a multimodal LLM Qwen-VL-Plus to create vision anal- ysis data based on these key correction frames as shown in the right part of Figure 2. The first ty...\n",
      "  2. Score: 0.294 | Doc: CorrectNav__Self-Correction_Flywheel_Empowers_Vision-Language-Action\n",
      "__Navigatio | Chunk: 25\n",
      "     Content: on(K(i)); Qa(i) ←MLLM QA(K(i)); Add (T (i) c , Cap(i), Qa(i)) to Dnew; Dtrain ←Sample(Dnav) ∪Sample(Dnew); M ←Train(Dtrain, M); Model Structure Our CorrectNav consists of three modules: the Vision En-...\n",
      "  3. Score: 0.288 | Doc: From_Surface_to_Semantics__Semantic_Structure_Parsing_for_Table-Centric\n",
      "__Docume | Chunk: 95\n",
      "     Content: t doc- ument understanding with visualizations,” in 38th Advances in Neural Information Processing Systems (NeurIPS), 2024. [13] Z. Tang, Z. Yang, G. Wang, Y. Fang, Y. Liu, C. Zhu, M. Zeng, C. Zhang, ...\n",
      "\n",
      "🔍 Query: reinforcement learning\n",
      "  1. Score: 0.539 | Doc: Pass@k_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of\n",
      "__Large | Chunk: 159\n",
      "     Content: xplorations in deep reinforcement learning. ACM Trans. Intell. Syst. Technol., 12(3):35:1–35:21, 2021. [28] Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. ...\n",
      "  2. Score: 0.324 | Doc: SSRL__Self-Search_Reinforcement_Learning | Chunk: 0\n",
      "     Content: Preprint. SSRL: SELF-SEARCH REINFORCEMENT LEARNING Yuchen Fan1,3,∗ Kaiyan Zhang1,∗,† Heng Zhou3,∗ Yuxin Zuo1,3 Yanxu Chen1 Yu Fu4 Xinwei Long1 Xuekai Zhu2 Che Jiang1 Yuchen Zhang3 Li Kang3 Gang Chen5 ...\n",
      "  3. Score: 0.323 | Doc: Making_Qwen3_Think_in_Korean_with_Reinforcement_Learning | Chunk: 26\n",
      "     Content: soning and to correct any remaining deficiencies from Phase 1 – we turn to reinforcement learning (RL). In this phase, the model learns from trial and error, guided by explicit reward signals that we ...\n",
      "\n",
      "🔍 Query: graph neural networks\n",
      "  1. Score: 0.696 | Doc: Beyond_Semantic_Understanding__Preserving_Collaborative_Frequency\n",
      "__Components_i | Chunk: 97\n",
      "     Content: Zhang, Taoran Ji, Kaidi Fu, Liang Zhao, Fanglan Chen, Lingfei Wu, Charu Aggarwal, and Chang-Tien Lu. 2022. Revisiting the role of similarity based graph filtering in graph neural networks. In Proceedi...\n",
      "  2. Score: 0.391 | Doc: Beyond_Semantic_Understanding__Preserving_Collaborative_Frequency\n",
      "__Components_i | Chunk: 132\n",
      "     Content: e with lower computational cost. • BSARec [33]: Introduces an attentive inductive bias mechanism that goes beyond self-attention, incorporating frequency-aware components to better capture sequential ...\n",
      "  3. Score: 0.340 | Doc: Beyond_Semantic_Understanding__Preserving_Collaborative_Frequency\n",
      "__Components_i | Chunk: 109\n",
      "     Content: e processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). 188–197. [28] Hoang NT and Takanori Maehara. 2019. Revisiting graph neural networks: All we have ...\n",
      "\n",
      "🔍 Query: self-supervised learning\n",
      "  1. Score: 0.514 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 54\n",
      "     Content: _modules/segmentation_models_pytorch/deeplabv3/model.html 7For the rest of the paper, we will refer to self-supervised learning when we utilize the unlabeled set for training and unsupervised learning...\n",
      "  2. Score: 0.483 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 58\n",
      "     Content: d the Adam optimizer for loss calculation and optimization, respectively. Due to time complexity considerations, we only experiment with a learning rate of 0.0001. In all cases, we stop training if th...\n",
      "  3. Score: 0.301 | Doc: Learning_from_Natural_Language_Feedback_for_Personalized_Question\n",
      "__Answering | Chunk: 53\n",
      "     Content: ved responses can provide useful supervision for training the next policy model 𝜋𝜃𝑡. Accordingly, the optimization objective of the feedback model should be designed to encourage the generation of fee...\n",
      "\n",
      "🔍 Query: unsupervised learning\n",
      "  1. Score: 0.496 | Doc: Layer-Wise_Perturbations_via_Sparse_Autoencoders_for_Adversarial_Text\n",
      "__Generati | Chunk: 5\n",
      "     Content: ese advancements, a persistent challenge remains: the generation of adversarial examples that can bypass these defenses, exposing potential vulnerabilities in NLP systems. One promising avenue for imp...\n",
      "  2. Score: 0.293 | Doc: A_Survey_on_Diffusion_Language_Models | Chunk: 158\n",
      "     Content: lternative to dominant AR approaches. Different from previously discussed D-DiT, UniDisc em- ploys an entire masked diffusion process jointly on text and image tokens with full attention, learning to ...\n",
      "  3. Score: 0.293 | Doc: Efficient_Forward-Only_Data_Valuation_for_Pretrained_LLMs_and_VLMs | Chunk: 84\n",
      "     Content: elabeling. In International Conference on Learning Representations. Kwon, Y.; Wu, E.; Wu, K.; and Zou, J. 2024. Datainf: Effi- ciently estimating data influence in lora-tuned llms and dif- fusion mode...\n",
      "\n",
      "🔍 Query: supervised learning\n",
      "  1. Score: 0.297 | Doc: Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of\n",
      "__Gloss_A | Chunk: 95\n",
      "     Content: y determined through cross-validation. Notably, Laplacian SVM [24] is a well-known model that combines a Laplacian regularizer with the Hinge loss. Additionally, one can consider Label Propagation as ...\n",
      "  2. Score: 0.294 | Doc: Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of\n",
      "__Gloss_A | Chunk: 255\n",
      "     Content: networks, vol. 20, no. 1, pp. 61–80, 2008. [24] S. Melacci and M. Belkin, “Laplacian support vector machines trained in the primal.” Journal of Machine Learning Research, vol. 12, no. 3, 2011. [25] S....\n",
      "  3. Score: 0.292 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 78\n",
      "     Content: ation approach, which ultimately provided superior results. To address the challenge of limited labeled data, we employed pseudo-labeling techniques to aug- ment our training set, resulting in notable...\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: 测试查询\n",
    "test_queries(hindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc1b9b-dbb1-4381-b417-9035a9290d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
