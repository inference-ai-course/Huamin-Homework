{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb63cab-19a8-4ee4-a3d0-06376999a042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯¼å…¥å®Œæˆï¼é…ç½®å°±ç»ªã€‚\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: å¯¼å…¥å’Œé…ç½®\n",
    "import sqlite3\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from fastapi import FastAPI\n",
    "import uvicorn\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "from contextlib import asynccontextmanager\n",
    "import nest_asyncio\n",
    "import requests\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# ä¿®å¤äº‹ä»¶å¾ªç¯é—®é¢˜\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# -----------------------------\n",
    "# å…¨å±€é…ç½®\n",
    "# -----------------------------\n",
    "PDF_DIR = Path(\"arxiv_pdfs\")\n",
    "TXT_DIR = Path(\"arxiv_texts\")  # ä½¿ç”¨week4ä½œä¸šå·²ç»ç”Ÿæˆçš„æ–‡æœ¬æ–‡ä»¶\n",
    "DB_PATH = \"hybrid_search.db\"\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# ç¡®ä¿ç›®å½•å­˜åœ¨\n",
    "PDF_DIR.mkdir(exist_ok=True)\n",
    "TXT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"å¯¼å…¥å®Œæˆï¼é…ç½®å°±ç»ªã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fdcc57-769c-4706-854b-171a03eba527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡æœ¬å¤„ç†å‡½æ•°å®šä¹‰å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: æ–‡æœ¬å¤„ç†å‡½æ•°ï¼ˆä¸ä½¿ç”¨fitzï¼‰\n",
    "def extract_metadata_from_filename(filename):\n",
    "    \"\"\"ä»æ–‡ä»¶åä¸­æå–å…ƒæ•°æ®\"\"\"\n",
    "    # ç§»é™¤æ–‡ä»¶æ‰©å±•å\n",
    "    name_without_ext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # æ ¹æ®å®é™…æ–‡ä»¶åæ ¼å¼è¿›è¡Œè°ƒæ•´\n",
    "    \n",
    "    # ç¤ºä¾‹ï¼šç®€å•çš„åˆ†å‰²æ–¹å¼\n",
    "    parts = name_without_ext.split('_')\n",
    "    \n",
    "    metadata = {\n",
    "        \"title\": parts[0] if len(parts) > 0 else \"æœªçŸ¥æ ‡é¢˜\",\n",
    "        \"authors\": parts[1] if len(parts) > 1 else \"æœªçŸ¥ä½œè€…\",\n",
    "        \"year\": int(parts[2]) if len(parts) > 2 and parts[2].isdigit() else 2023,\n",
    "        \"abstract\": \"\",  # é»˜è®¤ç©ºæ‘˜è¦\n",
    "        \"keywords\": \"\"   # é»˜è®¤ç©ºå…³é”®è¯\n",
    "    }\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def load_text_from_file(txt_file: Path) -> str:\n",
    "    \"\"\"ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½å†…å®¹\"\"\"\n",
    "    try:\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(doc_id: str, text: str, chunk_size=500, overlap=100) -> list:\n",
    "    \"\"\"æ–‡æœ¬åˆ†å—\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    chunk_id = 0\n",
    "    while i < len(words):\n",
    "        chunk_words = words[i:i + chunk_size]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        chunks.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"text\": chunk_text\n",
    "        })\n",
    "        i += chunk_size - overlap\n",
    "        chunk_id += 1\n",
    "    return chunks\n",
    "\n",
    "print(\"æ–‡æœ¬å¤„ç†å‡½æ•°å®šä¹‰å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec03bd-a464-4a80-aeed-65771d4f125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: æ•°æ®åº“åˆå§‹åŒ–\n",
    "def init_database():\n",
    "    \"\"\"åˆå§‹åŒ–æ•°æ®åº“ï¼Œåˆ›å»ºå¿…è¦çš„è¡¨\"\"\"\n",
    "    conn = sqlite3.connect('document_search.db')\n",
    "    \n",
    "    # åˆ›å»ºæ–‡æ¡£è¡¨ - ä¸ºä¸æ’å…¥è¯­å¥åŒ¹é…çš„ç»“æ„\n",
    "    conn.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS documents (\n",
    "        doc_id TEXT PRIMARY KEY,\n",
    "        title TEXT NOT NULL,\n",
    "        authors TEXT,\n",
    "        year INTEGER,\n",
    "        abstract TEXT,\n",
    "        keywords TEXT,\n",
    "        file_path TEXT NOT NULL,\n",
    "        upload_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # åˆ›å»ºFTS5è™šæ‹Ÿè¡¨ç”¨äºå…¨æ–‡æœç´¢\n",
    "    conn.execute(\"\"\"\n",
    "    CREATE VIRTUAL TABLE IF NOT EXISTS doc_chunks USING fts5(\n",
    "        chunk_id UNINDEXED,\n",
    "        content,\n",
    "        doc_id UNINDEXED,\n",
    "        title\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19f5047a-26b4-4609-a375-6ffad205cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: æ•°æ®åº“å¡«å……å‡½æ•°\n",
    "def split_text_into_chunks(text, chunk_size=512, overlap=50):\n",
    "    \"\"\"\n",
    "    å°†æ–‡æœ¬åˆ†å‰²æˆé‡å çš„å—\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        if end > text_length:\n",
    "            end = text_length\n",
    "        \n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # ç§»åŠ¨èµ·å§‹ä½ç½®ï¼Œè€ƒè™‘é‡å \n",
    "        start += chunk_size - overlap\n",
    "        \n",
    "        # ç¡®ä¿ä¸ä¼šæ— é™å¾ªç¯\n",
    "        if start >= text_length:\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "def populate_database(conn):\n",
    "    \"\"\"ä»æ–‡æœ¬æ–‡ä»¶å¡«å……æ•°æ®åº“\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # è·å–æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶\n",
    "    txt_files = glob.glob(os.path.join(TXT_DIR, \"*.txt\"))\n",
    "    print(f\"æ‰¾åˆ° {len(txt_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶\")\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        # ä»æ–‡ä»¶åæå–æ–‡æ¡£IDï¼ˆä¸å«æ‰©å±•åï¼‰\n",
    "        doc_id = os.path.splitext(os.path.basename(txt_file))[0]\n",
    "        \n",
    "        # ä»æ–‡ä»¶åæå–å…ƒæ•°æ®\n",
    "        metadata = extract_metadata_from_filename(doc_id)\n",
    "        \n",
    "        # æ’å…¥æ–‡æ¡£å…ƒæ•°æ® - æ·»åŠ  file_path å­—æ®µ\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO documents (doc_id, title, authors, year, abstract, keywords, file_path)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", (doc_id, metadata[\"title\"], metadata[\"authors\"], \n",
    "              metadata[\"year\"], metadata[\"abstract\"], \"\", txt_file))  # æ·»åŠ  txt_file ä½œä¸º file_path\n",
    "        \n",
    "        # åŠ è½½æ–‡æœ¬å†…å®¹\n",
    "        text = load_text_from_file(txt_file)\n",
    "        \n",
    "        # åˆ†å‰²æ–‡æœ¬ä¸ºå—\n",
    "        chunks = split_text_into_chunks(text)\n",
    "        \n",
    "        # æ’å…¥æ–‡æœ¬å—åˆ°FTSè¡¨\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO doc_chunks (chunk_id, content, doc_id, title)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "            \"\"\", (i, chunk, doc_id, metadata[\"title\"]))\n",
    "    \n",
    "    conn.commit()\n",
    "    print(f\"æˆåŠŸå¤„ç† {len(txt_files)} ä¸ªæ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fb7593c-8735-4bbd-911e-dd0d7aaa62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 4ï¸âƒ£ æ··åˆç´¢å¼•ç±»\n",
    "# -----------------------------\n",
    "class HybridIndex:\n",
    "    def __init__(self, conn, embed_model=\"all-MiniLM-L6-v2\"):\n",
    "        self.conn = conn\n",
    "        self.embedder = SentenceTransformer(embed_model)\n",
    "        self.index = None\n",
    "        self.bm25 = None\n",
    "        self.chunk_data = []\n",
    "        self.chunk_texts = []\n",
    "\n",
    "    def build_indices(self):\n",
    "        \"\"\"æ„å»ºFAISSå’ŒBM25ç´¢å¼•\"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"SELECT doc_id, chunk_id, content FROM doc_chunks\")\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        embeddings, self.chunk_data, self.chunk_texts = [], [], []\n",
    "        \n",
    "        for doc_id, chunk_id, content in rows:\n",
    "            vec = self.embedder.encode(content, convert_to_numpy=True)\n",
    "            embeddings.append(vec)\n",
    "            self.chunk_data.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"content\": content\n",
    "            })\n",
    "            self.chunk_texts.append(content)\n",
    "        \n",
    "        if embeddings:\n",
    "            embeddings = np.array(embeddings).astype(\"float32\")\n",
    "            dim = embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatL2(dim)\n",
    "            self.index.add(embeddings)\n",
    "            \n",
    "            tokenized_docs = [doc.split() for doc in self.chunk_texts]\n",
    "            self.bm25 = BM25Okapi(tokenized_docs)\n",
    "            \n",
    "            print(f\"Built indices with {len(embeddings)} chunks\")\n",
    "        else:\n",
    "            print(\"Warning: No embeddings created\")\n",
    "\n",
    "    def faiss_search(self, query, k=3):\n",
    "        if self.index is None:\n",
    "            return []\n",
    "        \n",
    "        q_vec = self.embedder.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "        D, I = self.index.search(q_vec, k)\n",
    "        \n",
    "        results = []\n",
    "        for i, d in zip(I[0], D[0]):\n",
    "            if i < len(self.chunk_data):\n",
    "                similarity = float(1 / (1 + d))\n",
    "                chunk_info = self.chunk_data[i]\n",
    "                results.append({\n",
    "                    \"doc_id\": chunk_info[\"doc_id\"],\n",
    "                    \"chunk_id\": chunk_info[\"chunk_id\"],\n",
    "                    \"score\": similarity,\n",
    "                    \"content\": chunk_info[\"content\"][:200] + \"...\"\n",
    "                })\n",
    "        return results\n",
    "\n",
    "    def fts5_search(self, query, k=3):\n",
    "        cursor = self.conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT doc_id, chunk_id, content \n",
    "            FROM doc_chunks \n",
    "            WHERE doc_chunks MATCH ? \n",
    "            ORDER BY rank\n",
    "            LIMIT ?\n",
    "        \"\"\", (query, k))\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, chunk_id, content in cursor.fetchall():\n",
    "            results.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"score\": 0.5,\n",
    "                \"content\": content[:200] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def bm25_search(self, query, k=3):\n",
    "        if not self.bm25:\n",
    "            return []\n",
    "        \n",
    "        tokenized_query = query.split()\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        top_indices = np.argsort(scores)[::-1][:k]\n",
    "        results = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if idx < len(self.chunk_data):\n",
    "                normalized_score = float(min(scores[idx] / 20, 1.0))\n",
    "                chunk_info = self.chunk_data[idx]\n",
    "                results.append({\n",
    "                    \"doc_id\": chunk_info[\"doc_id\"],\n",
    "                    \"chunk_id\": chunk_info[\"chunk_id\"],\n",
    "                    \"score\": normalized_score,\n",
    "                    \"content\": chunk_info[\"content\"][:200] + \"...\"\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def hybrid_search(self, query, k=3, alpha=0.6, use_fts5=False):\n",
    "        vector_results = self.faiss_search(query, k*2)\n",
    "        if use_fts5:\n",
    "            keyword_results = self.fts5_search(query, k*2)\n",
    "        else:\n",
    "            keyword_results = self.bm25_search(query, k*2)\n",
    "        \n",
    "        def get_key(result):\n",
    "            return f\"{result['doc_id']}_{result['chunk_id']}\"\n",
    "        \n",
    "        combined = {}\n",
    "        \n",
    "        for result in vector_results:\n",
    "            key = get_key(result)\n",
    "            combined[key] = {**result, \"vector_score\": result[\"score\"], \"keyword_score\": 0.0}\n",
    "        \n",
    "        for result in keyword_results:\n",
    "            key = get_key(result)\n",
    "            if key in combined:\n",
    "                combined[key][\"keyword_score\"] = result[\"score\"]\n",
    "            else:\n",
    "                combined[key] = {**result, \"vector_score\": 0.0, \"keyword_score\": result[\"score\"]}\n",
    "        \n",
    "        final_results = []\n",
    "        for key, result in combined.items():\n",
    "            hybrid_score = float(alpha * result[\"vector_score\"] + (1 - alpha) * result[\"keyword_score\"])\n",
    "            final_results.append({\n",
    "                \"doc_id\": result[\"doc_id\"],\n",
    "                \"chunk_id\": result[\"chunk_id\"],\n",
    "                \"score\": hybrid_score,\n",
    "                \"vector_score\": result[\"vector_score\"],\n",
    "                \"keyword_score\": result[\"keyword_score\"],\n",
    "                \"content\": result[\"content\"]\n",
    "            })\n",
    "        \n",
    "        final_results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return final_results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d6f06b3-39ca-4565-8799-5253cf875c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 5ï¸âƒ£ Jupyterä¸“ç”¨å‡½æ•°\n",
    "# -----------------------------\n",
    "def initialize_system():\n",
    "    \"\"\"åœ¨Jupyterä¸­åˆå§‹åŒ–æ•´ä¸ªç³»ç»Ÿ\"\"\"\n",
    "    print(\"Initializing database...\")\n",
    "    conn = init_database()\n",
    "    print(\"Populating database from PDF files...\")\n",
    "    populate_database(conn)\n",
    "    print(\"Building search indices...\")\n",
    "    hindex = HybridIndex(conn)\n",
    "    hindex.build_indices()\n",
    "    print(\"System initialized successfully!\")\n",
    "    return conn, hindex\n",
    "\n",
    "def test_queries(hindex, queries=None):\n",
    "    \"\"\"æµ‹è¯•æŸ¥è¯¢\"\"\"\n",
    "    if queries is None:\n",
    "        queries = [\n",
    "            \"machine translation\", \"transformer\", \"neural network\",\n",
    "            \"deep learning\", \"artificial intelligence\", \"natural language processing\",\n",
    "            \"computer vision\", \"reinforcement learning\", \"graph neural networks\",\n",
    "            \"self-supervised learning\", \"unsupervised learning\", \"supervised learning\",\n",
    "        ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nğŸ” Query: {query}\")\n",
    "        results = hindex.hybrid_search(query, 3)\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"  {i}. Score: {result['score']:.3f} | Doc: {result['doc_id']} | Chunk: {result['chunk_id']}\")\n",
    "            print(f\"     Content: {result['content']}\")\n",
    "\n",
    "def run_server_in_background():\n",
    "    \"\"\"åœ¨åå°è¿è¡ŒæœåŠ¡å™¨ï¼ˆJupyterä¸“ç”¨ï¼‰\"\"\"\n",
    "    import threading\n",
    "    \n",
    "    def start_server():\n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "        server = uvicorn.Server(config)\n",
    "        server.run()\n",
    "    \n",
    "    server_thread = threading.Thread(target=start_server, daemon=True)\n",
    "    server_thread.start()\n",
    "    print(\"Server started in background on http://localhost:8000\")\n",
    "    return server_thread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "db7b5f28-e85c-44a1-b73a-675ccf45ea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing database...\n",
      "æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼\n",
      "Populating database from PDF files...\n",
      "æ‰¾åˆ° 50 ä¸ªæ–‡æœ¬æ–‡ä»¶\n",
      "æˆåŠŸå¤„ç† 50 ä¸ªæ–‡ä»¶\n",
      "Building search indices...\n",
      "Built indices with 7709 chunks\n",
      "System initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: æ‰§è¡Œåˆå§‹åŒ–\n",
    "conn, hindex = initialize_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df225e47-ee5a-410a-a8d4-c2b7362f8901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Query: machine translation\n",
      "  1. Score: 0.568 | Doc: Estimating_Machine_Translation_Difficulty | Chunk: 78\n",
      "     Content: t using the quality of the translations it produces. While we acknowledge that translation difficulty should ideally be an in- trinsic property of the source â€“ independent of any specific translation ...\n",
      "  2. Score: 0.546 | Doc: Estimating_Machine_Translation_Difficulty | Chunk: 17\n",
      "     Content: ion, meaning that the same source text might be more difficult to translate into one lan- guage than into another. Moreover, translation diffi- culty might not be uniform across translators. It can va...\n",
      "  3. Score: 0.388 | Doc: Estimating_Machine_Translation_Difficulty | Chunk: 0\n",
      "     Content: Estimating Machine Translation Difficulty Lorenzo Proietti1,âˆ— Stefano Perrella1,âˆ— VilÃ©m Zouhar2,âˆ— Roberto Navigli1 Tom Kocmi3 1Sapienza NLP Group, Sapienza University of Rome 2ETH Zurich 3 Cohere {lpr...\n",
      "\n",
      "ğŸ” Query: transformer\n",
      "  1. Score: 0.316 | Doc: Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of\n",
      "__Gloss_A | Chunk: 59\n",
      "     Content: discuss the theoretical underpinnings of our approach in chapter 2, with the transformer serving as the primary focal point. Discuss the works that have already Chapter 1. Introduction 7 been done on ...\n",
      "  2. Score: 0.313 | Doc: eDIF__A_European_Deep_Inference_Fabric_for_Remote_Interpretability_of\n",
      "__LLM | Chunk: 91\n",
      "     Content: om/TransformerLensOrg/ TransformerLens [2] C. Tang, B. Lake, and M. Jazayeri, â€œAn explainable transformer circuit for compositional generalization,â€ arXiv preprint arXiv:2502.15801, Feb. 2025. [Online...\n",
      "  3. Score: 0.310 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 25\n",
      "     Content: th content-based and location- basedâ€”addressing the challenges of training a transformer from scratch with fewer than 100,000 data points. By applying data augmentations exclusively during training, t...\n",
      "\n",
      "ğŸ” Query: neural network\n",
      "  1. Score: 0.495 | Doc: Computational_Economics_in_Large_Language_Models__Exploring_Model\n",
      "__Behavior_and | Chunk: 36\n",
      "     Content: ernal components of a neural network itself. This perspective is supported by the information bottleneck principle, which posits that any learning system should optimally trade off between compressing...\n",
      "  2. Score: 0.490 | Doc: Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of\n",
      "__Gloss_A | Chunk: 80\n",
      "     Content: morphism test[20]. Regardless of the motive, a GNNâ€™s distinguishing fea- ture is that it employs message passing between neighbourhood nodes and update this information using neural network similar to...\n",
      "  3. Score: 0.304 | Doc: Computational_Economics_in_Large_Language_Models__Exploring_Model\n",
      "__Behavior_and | Chunk: 43\n",
      "     Content: icient activations. head is considered an agent. For the feed-forward network (FFN), each neuron (or a group of neurons) can be modeled as an agent. These agents are responsible for processing informa...\n",
      "\n",
      "ğŸ” Query: deep learning\n",
      "  1. Score: 0.314 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 4\n",
      "     Content: ter recognition proves to be a complex endeavor since the shapes of letters are influenced by numerous factors, including the writerâ€™s style, the type of pen used, the surface material, and the direct...\n",
      "  2. Score: 0.307 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 28\n",
      "     Content: Neural Networks (CNNs) can be effective. For instance, a Bidirectional LSTM possesses the advantage of capturing more sequential information compared to its unidirectional counterpart [GS05], making i...\n",
      "  3. Score: 0.296 | Doc: Understanding_Textual_Emotion_Through_Emoji_Prediction | Chunk: 13\n",
      "     Content:  2.1. Design Choices and Implementation To conduct this project and explore various approaches, four models are created using different architectures: BERT, a feedforward neural network, a transformer...\n",
      "\n",
      "ğŸ” Query: artificial intelligence\n",
      "  1. Score: 0.309 | Doc: Computational_Economics_in_Large_Language_Models__Exploring_Model\n",
      "__Behavior_and | Chunk: 35\n",
      "     Content: verse engineeringâ€ of game theory; it focuses on designing the rules of a game to incentivize self-interested agents to behave in a way that achieves a desirable system-wide outcome [46]. This framewo...\n",
      "  2. Score: 0.297 | Doc: Making_Qwen3_Think_in_Korean_with_Reinforcement_Learning | Chunk: 80\n",
      "     Content: ng an AI â€œthinkâ€ in the userâ€™s language opens the door for more inclusive AI systems globally. Rather than treating English as the default language of intelligence, our work suggests that with focused...\n",
      "  3. Score: 0.295 | Doc: mSCoRe__a_$M$ultilingual_and_Scalable_Benchmark_for_$S$kill-based\n",
      "__$Co$mmonsens | Chunk: 215\n",
      "     Content:  AI's ability to utilize commonsense reasoning to arrive at the correct answer. The goal is to test and enhance the AI's understanding of cultural norms and behaviors in a specific setting. Provide th...\n",
      "\n",
      "ğŸ” Query: natural language processing\n",
      "  1. Score: 0.341 | Doc: LaajMeter__A_Framework_for_LaaJ_Evaluation | Chunk: 4\n",
      "     Content: ities of Natural Language Processing (NLP), making a wide range of tasks such as translation, summarization, question answering, and code generation not only feasible but scalable and accessible. Thei...\n",
      "  2. Score: 0.320 | Doc: Improving_Generative_Cross-lingual_Aspect-Based_Sentiment_Analysis_with\n",
      "__Constr | Chunk: 175\n",
      "     Content: logy.org/N18-1053 2. Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., Lovenia, H., Ji, Z., Yu, T., Chung, W., Do, Q.V., Xu, Y., Fung, P.: A multitask, multilin- gual, multimodal evaluat...\n",
      "  3. Score: 0.318 | Doc: Large_Language_Models_for_Summarizing_Czech_Historical_Documents_and\n",
      "__Beyond | Chunk: 61\n",
      "     Content:  J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow- icz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M., Lhoes...\n",
      "\n",
      "ğŸ” Query: computer vision\n",
      "  1. Score: 0.296 | Doc: CorrectNav__Self-Correction_Flywheel_Empowers_Vision-Language-Action\n",
      "__Navigatio | Chunk: 43\n",
      "     Content: tion keyframes {K1, K2, K3}. We then lever- age a multimodal LLM Qwen-VL-Plus to create vision anal- ysis data based on these key correction frames as shown in the right part of Figure 2. The first ty...\n",
      "  2. Score: 0.294 | Doc: CorrectNav__Self-Correction_Flywheel_Empowers_Vision-Language-Action\n",
      "__Navigatio | Chunk: 25\n",
      "     Content: on(K(i)); Qa(i) â†MLLM QA(K(i)); Add (T (i) c , Cap(i), Qa(i)) to Dnew; Dtrain â†Sample(Dnav) âˆªSample(Dnew); M â†Train(Dtrain, M); Model Structure Our CorrectNav consists of three modules: the Vision En-...\n",
      "  3. Score: 0.288 | Doc: From_Surface_to_Semantics__Semantic_Structure_Parsing_for_Table-Centric\n",
      "__Docume | Chunk: 95\n",
      "     Content: t doc- ument understanding with visualizations,â€ in 38th Advances in Neural Information Processing Systems (NeurIPS), 2024. [13] Z. Tang, Z. Yang, G. Wang, Y. Fang, Y. Liu, C. Zhu, M. Zeng, C. Zhang, ...\n",
      "\n",
      "ğŸ” Query: reinforcement learning\n",
      "  1. Score: 0.539 | Doc: Pass@k_Training_for_Adaptively_Balancing_Exploration_and_Exploitation_of\n",
      "__Large | Chunk: 159\n",
      "     Content: xplorations in deep reinforcement learning. ACM Trans. Intell. Syst. Technol., 12(3):35:1â€“35:21, 2021. [28] Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. ...\n",
      "  2. Score: 0.324 | Doc: SSRL__Self-Search_Reinforcement_Learning | Chunk: 0\n",
      "     Content: Preprint. SSRL: SELF-SEARCH REINFORCEMENT LEARNING Yuchen Fan1,3,âˆ— Kaiyan Zhang1,âˆ—,â€  Heng Zhou3,âˆ— Yuxin Zuo1,3 Yanxu Chen1 Yu Fu4 Xinwei Long1 Xuekai Zhu2 Che Jiang1 Yuchen Zhang3 Li Kang3 Gang Chen5 ...\n",
      "  3. Score: 0.323 | Doc: Making_Qwen3_Think_in_Korean_with_Reinforcement_Learning | Chunk: 26\n",
      "     Content: soning and to correct any remaining deficiencies from Phase 1 â€“ we turn to reinforcement learning (RL). In this phase, the model learns from trial and error, guided by explicit reward signals that we ...\n",
      "\n",
      "ğŸ” Query: graph neural networks\n",
      "  1. Score: 0.696 | Doc: Beyond_Semantic_Understanding__Preserving_Collaborative_Frequency\n",
      "__Components_i | Chunk: 97\n",
      "     Content: Zhang, Taoran Ji, Kaidi Fu, Liang Zhao, Fanglan Chen, Lingfei Wu, Charu Aggarwal, and Chang-Tien Lu. 2022. Revisiting the role of similarity based graph filtering in graph neural networks. In Proceedi...\n",
      "  2. Score: 0.391 | Doc: Beyond_Semantic_Understanding__Preserving_Collaborative_Frequency\n",
      "__Components_i | Chunk: 132\n",
      "     Content: e with lower computational cost. â€¢ BSARec [33]: Introduces an attentive inductive bias mechanism that goes beyond self-attention, incorporating frequency-aware components to better capture sequential ...\n",
      "  3. Score: 0.340 | Doc: Beyond_Semantic_Understanding__Preserving_Collaborative_Frequency\n",
      "__Components_i | Chunk: 109\n",
      "     Content: e processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). 188â€“197. [28] Hoang NT and Takanori Maehara. 2019. Revisiting graph neural networks: All we have ...\n",
      "\n",
      "ğŸ” Query: self-supervised learning\n",
      "  1. Score: 0.514 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 54\n",
      "     Content: _modules/segmentation_models_pytorch/deeplabv3/model.html 7For the rest of the paper, we will refer to self-supervised learning when we utilize the unlabeled set for training and unsupervised learning...\n",
      "  2. Score: 0.483 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 58\n",
      "     Content: d the Adam optimizer for loss calculation and optimization, respectively. Due to time complexity considerations, we only experiment with a learning rate of 0.0001. In all cases, we stop training if th...\n",
      "  3. Score: 0.301 | Doc: Learning_from_Natural_Language_Feedback_for_Personalized_Question\n",
      "__Answering | Chunk: 53\n",
      "     Content: ved responses can provide useful supervision for training the next policy model ğœ‹ğœƒğ‘¡. Accordingly, the optimization objective of the feedback model should be designed to encourage the generation of fee...\n",
      "\n",
      "ğŸ” Query: unsupervised learning\n",
      "  1. Score: 0.496 | Doc: Layer-Wise_Perturbations_via_Sparse_Autoencoders_for_Adversarial_Text\n",
      "__Generati | Chunk: 5\n",
      "     Content: ese advancements, a persistent challenge remains: the generation of adversarial examples that can bypass these defenses, exposing potential vulnerabilities in NLP systems. One promising avenue for imp...\n",
      "  2. Score: 0.293 | Doc: A_Survey_on_Diffusion_Language_Models | Chunk: 158\n",
      "     Content: lternative to dominant AR approaches. Different from previously discussed D-DiT, UniDisc em- ploys an entire masked diffusion process jointly on text and image tokens with full attention, learning to ...\n",
      "  3. Score: 0.293 | Doc: Efficient_Forward-Only_Data_Valuation_for_Pretrained_LLMs_and_VLMs | Chunk: 84\n",
      "     Content: elabeling. In International Conference on Learning Representations. Kwon, Y.; Wu, E.; Wu, K.; and Zou, J. 2024. Datainf: Effi- ciently estimating data influence in lora-tuned llms and dif- fusion mode...\n",
      "\n",
      "ğŸ” Query: supervised learning\n",
      "  1. Score: 0.297 | Doc: Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of\n",
      "__Gloss_A | Chunk: 95\n",
      "     Content: y determined through cross-validation. Notably, Laplacian SVM [24] is a well-known model that combines a Laplacian regularizer with the Hinge loss. Additionally, one can consider Label Propagation as ...\n",
      "  2. Score: 0.294 | Doc: Continuous_Bangla_Sign_Language_Translation__Mitigating_the_Expense_of\n",
      "__Gloss_A | Chunk: 255\n",
      "     Content: networks, vol. 20, no. 1, pp. 61â€“80, 2008. [24] S. Melacci and M. Belkin, â€œLaplacian support vector machines trained in the primal.â€ Journal of Machine Learning Research, vol. 12, no. 3, 2011. [25] S....\n",
      "  3. Score: 0.292 | Doc: Improving_OCR_for_Historical_Texts_of_Multiple_Languages | Chunk: 78\n",
      "     Content: ation approach, which ultimately provided superior results. To address the challenge of limited labeled data, we employed pseudo-labeling techniques to aug- ment our training set, resulting in notable...\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: æµ‹è¯•æŸ¥è¯¢\n",
    "test_queries(hindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effc1b9b-dbb1-4381-b417-9035a9290d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
