{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60bc6e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.55.4)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6f27ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62fe03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device == \"mps\" else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67027143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:23<00:00, 11.71s/it]\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Scott Lai? Scott Lai, also known as Scott Lai (Chinese: 賴昭霖; pinyin: Lài Zhāolín) or simply Lai, is a Chinese-American professional poker player and entrepreneur. He is known for his success in the World Series of Poker (WSOP) and other high-stakes poker tournaments.\n",
      "\n",
      "Key points about Scott Lai:\n",
      "\n",
      "1. Born in 1982 in Hong Kong to Taiwanese parents.\n",
      "2. Moved to the United States at a young age and grew up in California.\n",
      "3. Started playing poker seriously in college.\n",
      "4. Has won multiple WSOP bracelets, including the $50,000 No Limit Hold'em Tournament in 2013, which is one of the largest poker tournaments in the world.\n",
      "5. Has participated in several high-profile poker events, including the 2015 WSOP Main Event where he finished 7th for a prize of $666,666.\n",
      "6. Has worked as a poker commentator on ESPN and other networks.\n",
      "7. Has invested in various tech companies, including a venture capital firm called \"The Lai Ventures.\"\n",
      "8. Known for his calm demeanor and strategic approach to poker, often described as having a \"calm\n"
     ]
    }
   ],
   "source": [
    "if \"ask_llm\" not in globals():\n",
    "    from transformers import pipeline\n",
    "    ask_llm = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=\"./my-qwen\",\n",
    "        tokenizer=\"./my-qwen\",\n",
    "        device=device,\n",
    "        torch_dtype=dtype\n",
    "    )\n",
    "\n",
    "print(ask_llm(\"Who is Scott Lai?\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145dc2e4",
   "metadata": {},
   "source": [
    "As you can see here, the model has no idea who I am from above response.\n",
    "\n",
    "Let's cook it!\n",
    "\n",
    "First, let's teach the model who I am. Here you can use your personal data to generate the exact format you will use for fine-turning base on your own data. You can use ChatGPT for this, just ask it to transfer your resume into the trainable json format with \"prompt\" and \"completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "869c27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 122\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset('json', data_files = \"scott_lai_resume_train.json\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e9dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is Scott Lai’s profession?',\n",
       " 'completion': 'AI Engineer and Data Scientist.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13cf6e",
   "metadata": {},
   "source": [
    "As you can see, here we return with the long text, but for fine-tuning we need the data to be small and precise chunks, more like here we apply the tokenization to take the text and split it into smaller chunks. Each chunk is called a token and it the smallest unit of meaning that LLMs work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da03887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\"\n",
    ")\n",
    "def preprocess(sample):\n",
    "    sample = sample['prompt']+ '\\n' + sample['completion']\n",
    "    print(sample)\n",
    "    tokenized = tokenizer(\n",
    "        sample,\n",
    "        max_length = 128,\n",
    "        truncation = True,\n",
    "        padding = \"max_length\"    \n",
    "    )\n",
    "\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "data = raw_data.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6aae75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 122\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e743f40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "436ab97f",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "now, let's move into the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90228ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c30e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    device_map=\"auto\",       # accelerate 会管理 GPU\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33200b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    num_train_epochs = 10, # we will go throught the dataset from start to finish 10 times\n",
    "    learning_rate=0.001, \n",
    "    logging_steps = 20, # we want to see the result in every 25 steps it runs \n",
    "    fp16 = True, # float point set to 16 to speed it up, set to \"True\" if you are on GPU\n",
    "    # per_device_train_batch_size=1,  # <-- 不设置默认为8\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args = train_args,\n",
    "    model = model, \n",
    "    train_dataset=data[\"train\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b17f16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 00:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.13470476493239403, metrics={'train_runtime': 53.4216, 'train_samples_per_second': 22.837, 'train_steps_per_second': 2.995, 'total_flos': 2602200748523520.0, 'train_loss': 0.13470476493239403, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed1aafcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my-qwen/tokenizer_config.json',\n",
       " './my-qwen/special_tokens_map.json',\n",
       " './my-qwen/chat_template.jinja',\n",
       " './my-qwen/vocab.json',\n",
       " './my-qwen/merges.txt',\n",
       " './my-qwen/added_tokens.json',\n",
       " './my-qwen/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "trainer.save_model(\"./my-qwen\")\n",
    "tokenizer.save_pretrained(\"./my-qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7449af4",
   "metadata": {},
   "source": [
    "Now let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82ef1abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:11<00:00,  5.67s/it]\n",
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Scott Lai?\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "\n",
    "# 删除旧的 pipeline 对象\n",
    "if \"ask_llm\" in globals():\n",
    "    del ask_llm\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 再加载一次\n",
    "from transformers import pipeline\n",
    "ask_llm = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=\"./my-qwen\",\n",
    "    tokenizer=\"./my-qwen\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(ask_llm(\"Who is Scott Lai?\")[0][\"generated_text\"])\n",
    "# print(ask_llm(\"Who is Scott Lai?\", max_new_tokens=20)[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
