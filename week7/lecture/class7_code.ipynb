{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 7: Comprehensive Alignment Comparison\n",
    "\n",
    "Welcome to the educational walkthrough for **PPO vs DPO vs GRPO** in LLM alignment!\n",
    "\n",
    "This series of notebooks will guide you step by step through the concepts, code, and experiments for comparing three major alignment methods for language models.\n",
    "\n",
    "**Key Topics:**\n",
    "- Preference data collection\n",
    "- PPO (Proximal Policy Optimization)\n",
    "- DPO (Direct Preference Optimization)\n",
    "- GRPO (Group Relative Policy Optimization)\n",
    "- Evaluation and comparison\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction & Setup\n",
    "\n",
    "This notebook sets up the environment and introduces the main configuration and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianghm/MLE_Training/week7/sft-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TRL library loaded successfully\n",
      "‚úÖ Gradio library loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Any\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "\n",
    "# Configuration\n",
    "USE_SMALL_MODELS = True\n",
    "SKIP_OLLAMA = True\n",
    "\n",
    "# Import libraries with error handling\n",
    "try:\n",
    "    from trl import PPOConfig, DPOConfig, GRPOConfig\n",
    "    TRL_AVAILABLE = True\n",
    "    print(\"‚úÖ TRL library loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f'‚ùå TRL not found: {e}')\n",
    "    TRL_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import gradio as gr\n",
    "    GRADIO_AVAILABLE = True\n",
    "    print(\"‚úÖ Gradio library loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå Gradio not found. Install with: pip install gradio\")\n",
    "    GRADIO_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Loading Preference Datasets\n",
    "\n",
    "In this step, we'll load and inspect the preference datasets that will be used for alignment training.\n",
    "Preference datasets are essential for training and evaluating alignment methods such as PPO, DPO, and GRPO.\n",
    "\n",
    "---\n",
    "\n",
    "## Function: `load_preference_datasets`\n",
    "\n",
    "This function loads a comprehensive set of preference data, including both sample data and (if available) external datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "class AlignmentComparisonManager:\n",
    "    def load_preference_datasets(self) -> Dict:\n",
    "        \"\"\"Step 1: Load comprehensive preference datasets\"\"\"\n",
    "        print(\"üìö STEP 1: LOADING PREFERENCE DATASETS\")\n",
    "        print(\"Creating comprehensive preference data for alignment training...\")\n",
    "        # Create high-quality preference data\n",
    "        preference_data = [\n",
    "            {\n",
    "                'prompt': 'How do you debug a memory leak in a Python application?',\n",
    "                'chosen': 'To debug memory leaks in Python: 1) Use memory profilers like memory_profiler or pympler, 2) Identify objects not being garbage collected, 3) Check for circular references, 4) Review global variables and caches, 5) Use weak references appropriately, 6) Monitor memory usage over time, 7) Use tools like objgraph to visualize object references.',\n",
    "                'rejected': \"Just restart the application when it uses too much memory. Memory leaks aren\\'t really a problem in Python.\"\n",
    "            },\n",
    "            {\n",
    "                'prompt': \"What's the difference between supervised and unsupervised learning?\",\n",
    "                'chosen': \"Supervised learning uses labeled training data where input-output pairs guide the algorithm to learn patterns for prediction. Examples include classification and regression. Unsupervised learning finds hidden patterns in unlabeled data without target outputs, such as clustering and dimensionality reduction. The key difference is the presence of target variables in supervised learning.\",\n",
    "                'rejected': \"Supervised learning is when someone supervises the computer while it learns. Unsupervised learning is when the computer learns by itself.\"\n",
    "            },\n",
    "            {\n",
    "                'prompt': 'How do you develop a go-to-market strategy for a new product?',\n",
    "                'chosen': 'A comprehensive GTM strategy includes: 1) Market research and customer segmentation, 2) Value proposition definition, 3) Competitive analysis, 4) Pricing strategy, 5) Distribution channel selection, 6) Marketing and sales strategies, 7) Success metrics and KPIs, 8) Launch timeline and milestones, 9) Risk assessment and contingency plans, 10) Post-launch optimization plan.',\n",
    "                'rejected': \"Just build the product and start selling it. If it's good, people will buy it. Marketing isn't that important.\"\n",
    "            },\n",
    "            {\n",
    "                'prompt': 'How do you handle underperformance in your team?',\n",
    "                'chosen': 'Address underperformance systematically: 1) Document specific performance gaps, 2) Have a direct, empathetic conversation to understand root causes, 3) Collaborate on a performance improvement plan with clear expectations, 4) Provide necessary resources and support, 5) Schedule regular check-ins, 6) Recognize improvements, 7) If no improvement, follow HR procedures.',\n",
    "                'rejected': 'Call them out in front of the team so everyone knows they need to improve. Public pressure usually motivates people.'\n",
    "            },\n",
    "            {\n",
    "                'prompt': 'How do you communicate complex technical concepts to non-technical stakeholders?',\n",
    "                'chosen': \"Effective technical communication involves: 1) Understanding your audience's background, 2) Using analogies and real-world examples, 3) Avoiding jargon, 4) Focusing on business impact, 5) Using visual aids, 6) Structuring information logically, 7) Checking for understanding, 8) Providing clear next steps.\",\n",
    "                'rejected': 'Dumb it down as much as possible and use simple words. Technical people just need to learn to explain things better.'\n",
    "            }\n",
    "        ]\n",
    "        # Try to load real datasets\n",
    "        datasets_info = {\n",
    "            'comprehensive_preferences': {\n",
    "                'dataset': Dataset.from_list(preference_data),\n",
    "                'description': 'Comprehensive multi-domain preference dataset',\n",
    "                'size': len(preference_data)\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            print('  üì• Attempting to load real preference datasets...')\n",
    "            hh_dataset = load_dataset('Anthropic/hh-rlhf', split='train[:10]')\n",
    "            datasets_info['hh_rlhf'] = {\n",
    "                'dataset': hh_dataset,\n",
    "                'description': 'Anthropic HH-RLHF helpful and harmless',\n",
    "                'size': len(hh_dataset)\n",
    "            }\n",
    "            print(f'  ‚úÖ Loaded {len(hh_dataset)} samples from HH-RLHF')\n",
    "        except Exception as e:\n",
    "            print(f'  ‚ö†Ô∏è Could not load external datasets: {e}')\n",
    "        print(f'üìä Total datasets available: {len(datasets_info)}')\n",
    "        return datasets_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example: Load and Inspect Datasets\n",
    "\n",
    "Let's create an instance of the manager and load the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö STEP 1: LOADING PREFERENCE DATASETS\n",
      "Creating comprehensive preference data for alignment training...\n",
      "  üì• Attempting to load real preference datasets...\n",
      "  ‚úÖ Loaded 10 samples from HH-RLHF\n",
      "üìä Total datasets available: 2\n",
      "comprehensive_preferences: Comprehensive multi-domain preference dataset (size: 5)\n",
      "hh_rlhf: Anthropic HH-RLHF helpful and harmless (size: 10)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the manager and load datasets\n",
    "manager = AlignmentComparisonManager()\n",
    "datasets_info = manager.load_preference_datasets()\n",
    "\n",
    "# Show available datasets and their sizes\n",
    "for name, info in datasets_info.items():\n",
    "    print(f\"{name}: {info['description']} (size: {info['size']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Building the Annotation Platform\n",
    "\n",
    "In this step, we'll build an interactive annotation platform for collecting preference data.\n",
    "\n",
    "This platform allows users to compare two responses to a prompt and indicate which one they prefer, along with their reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "## Function: `create_annotation_platform`\n",
    "\n",
    "This function creates a Gradio interface for preference annotation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class AlignmentComparisonManager:\n",
    "    def __init__(self):\n",
    "        self.annotations = []\n",
    "\n",
    "    def create_annotation_platform(self):\n",
    "        \"\"\"Create preference annotation platform\"\"\"\n",
    "        def generate_responses(prompt, style_a=\"helpful\", style_b=\"brief\"):\n",
    "            templates = {\n",
    "                \"helpful\": f\"Here's a comprehensive approach to {prompt.lower()}: ...\",\n",
    "                \"brief\": f\"For {prompt.lower()}: ...\",\n",
    "                \"detailed\": f\"Regarding {prompt.lower()}, this requires careful consideration...\",\n",
    "                \"technical\": f\"To address {prompt.lower()}, implement systematic methodology...\"\n",
    "            }\n",
    "            return (templates.get(style_a, templates[\"helpful\"]), templates.get(style_b, templates[\"brief\"]))\n",
    "\n",
    "        def save_annotation(prompt, response_a, response_b, preference, reasoning):\n",
    "            annotation = {\n",
    "                \"prompt\": prompt,\n",
    "                \"response_a\": response_a,\n",
    "                \"response_b\": response_b,\n",
    "                \"chosen\": response_a if preference == \"Response A\" else response_b,\n",
    "                \"rejected\": response_b if preference == \"Response A\" else response_a,\n",
    "                \"reasoning\": reasoning,\n",
    "                \"timestamp\": str(np.datetime64('now'))\n",
    "            }\n",
    "            self.annotations.append(annotation)\n",
    "            with open(\"annotations.json\", \"w\") as f:\n",
    "                json.dump(self.annotations, f, indent=2)\n",
    "            return f\"‚úÖ Saved annotation #{len(self.annotations)}\"\n",
    "\n",
    "        with gr.Blocks(title=\"üéØ Preference Annotation Platform\") as demo:\n",
    "            gr.Markdown(\"# üéØ Preference Annotation Platform\")\n",
    "            gr.Markdown(\"Create preference data for alignment training\")\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    prompt_input = gr.Textbox(label=\"üìù Enter Prompt\", placeholder=\"How do you handle difficult conversations?\", lines=3)\n",
    "                    with gr.Row():\n",
    "                        style_a = gr.Dropdown([\"helpful\", \"detailed\", \"technical\", \"brief\"], label=\"Style A\", value=\"helpful\")\n",
    "                        style_b = gr.Dropdown([\"helpful\", \"detailed\", \"technical\", \"brief\"], label=\"Style B\", value=\"brief\")\n",
    "                    generate_btn = gr.Button(\"üöÄ Generate Responses\", variant=\"primary\")\n",
    "                with gr.Column():\n",
    "                    status_output = gr.Textbox(label=\"üìä Status\", interactive=False)\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"### üÖ∞Ô∏è Response A\")\n",
    "                    response_a_output = gr.Textbox(label=\"Response A\", lines=6, interactive=False)\n",
    "                with gr.Column():\n",
    "                    gr.Markdown(\"### üÖ±Ô∏è Response B\")\n",
    "                    response_b_output = gr.Textbox(label=\"Response B\", lines=6, interactive=False)\n",
    "            preference_radio = gr.Radio([\"Response A\", \"Response B\"], label=\"üëç Which response is better?\", value=None)\n",
    "            reasoning_input = gr.Textbox(label=\"üí≠ Reasoning\", placeholder=\"Why is this response better?\", lines=3)\n",
    "            save_btn = gr.Button(\"üíæ Save Annotation\", variant=\"secondary\")\n",
    "            generate_btn.click(fn=generate_responses, inputs=[prompt_input, style_a, style_b], outputs=[response_a_output, response_b_output])\n",
    "            save_btn.click(fn=save_annotation, inputs=[prompt_input, response_a_output, response_b_output, preference_radio, reasoning_input], outputs=[status_output])\n",
    "        return demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Usage: Launch the Annotation Platform\n",
    "\n",
    "To launch the interactive annotation platform, simply run the following code cell. This will open a Gradio interface in your browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "manager = AlignmentComparisonManager()\n",
    "demo = manager.create_annotation_platform()\n",
    "demo.launch()  # This will start the Gradio app for annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Preparing Datasets for Alignment Methods\n",
    "\n",
    "In this step, we'll prepare the datasets for different alignment methods: PPO, DPO, and GRPO.\n",
    "\n",
    "This step converts the loaded preference data into the formats required for each method.\n",
    "\n",
    "---\n",
    "\n",
    "## Function: `prepare_datasets`\n",
    "\n",
    "This function processes all available datasets and creates specialized datasets for each alignment method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "class AlignmentComparisonManager:\n",
    "    def __init__(self):\n",
    "        self.annotations = []\n",
    "\n",
    "    def prepare_datasets(self, datasets_info):\n",
    "        \"\"\"Prepare datasets for different alignment methods\"\"\"\n",
    "        print(\"\\nüîß STEP 3: PREPARING ALIGNMENT DATASETS\")\n",
    "        print(\"Converting data for PPO, DPO, and GRPO training...\")\n",
    "        all_preference_data = []\n",
    "        for dataset_name, info in datasets_info.items():\n",
    "            dataset = info['dataset']\n",
    "            print(f\"  üìä Processing {dataset_name} ({info['size']} samples)...\")\n",
    "            for example in dataset:\n",
    "                try:\n",
    "                    if 'chosen' in example and 'rejected' in example:\n",
    "                        preference_example = {\n",
    "                            'prompt': str(example.get('prompt', '')),\n",
    "                            'chosen': str(example['chosen']),\n",
    "                            'rejected': str(example['rejected'])\n",
    "                        }\n",
    "                    else:\n",
    "                        continue\n",
    "                    if all(len(str(preference_example[key]).strip()) > 10 for key in ['prompt', 'chosen', 'rejected']):\n",
    "                        all_preference_data.append(preference_example)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        # Add manual annotations\n",
    "        if self.annotations:\n",
    "            print(f\"  üìù Adding {len(self.annotations)} manual annotations...\")\n",
    "            for annotation in self.annotations:\n",
    "                preference_example = {\n",
    "                    'prompt': str(annotation['prompt']),\n",
    "                    'chosen': str(annotation['chosen']),\n",
    "                    'rejected': str(annotation['rejected'])\n",
    "                }\n",
    "                all_preference_data.append(preference_example)\n",
    "        datasets = {}\n",
    "        if all_preference_data:\n",
    "            datasets['dpo'] = Dataset.from_list(all_preference_data)\n",
    "            datasets['ppo'] = Dataset.from_list([{'query': item['prompt']} for item in all_preference_data])\n",
    "            datasets['grpo'] = Dataset.from_list([{'prompt': item['prompt']} for item in all_preference_data])\n",
    "            print(f\"  ‚úÖ Created datasets - DPO: {len(datasets['dpo'])}, PPO: {len(datasets['ppo'])}, GRPO: {len(datasets['grpo'])}\")\n",
    "        return datasets\n",
    "    \n",
    "        #üîß STEP 3: PREPARING ALIGNMENT DATASETS\")\n",
    "        print(\"Converting data for PPO, DPO, and GRPO training...\")\n",
    "        all_preference_data = []\n",
    "        for dataset_name, info in datasets_info.items():\n",
    "            dataset = info['dataset']\n",
    "            print(f\"  üìä Processing {dataset_name} ({info['size']} samples)...\")\n",
    "            for example in dataset:\n",
    "                try:\n",
    "                    if 'chosen' in example and 'rejected' in example:\n",
    "                        preference_example = {\n",
    "                            'prompt': str(example.get('prompt', '')),\n",
    "                            'chosen': str(example['chosen']),\n",
    "                            'rejected': str(example['rejected'])\n",
    "                        }\n",
    "                    else:\n",
    "                        continue\n",
    "                    if all(len(str(preference_example[key]).strip()) > 10 for key in ['prompt', 'chosen', 'rejected']):\n",
    "                        all_preference_data.append(preference_example)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        # Add manual annotations\n",
    "        if self.annotations:\n",
    "            print(f\"  üìù Adding {len(self.annotations)} manual annotations...\")\n",
    "            for annotation in self.annotations:\n",
    "                preference_example = {\n",
    "                    'prompt': str(annotation['prompt']),\n",
    "                    'chosen': str(annotation['chosen']),\n",
    "                    'rejected': str(annotation['rejected'])\n",
    "                }\n",
    "                all_preference_data.append(preference_example)\n",
    "        datasets = {}\n",
    "        if all_preference_data:\n",
    "            datasets['dpo'] = Dataset.from_list(all_preference_data)\n",
    "            datasets['ppo'] = Dataset.from_list([{'query': item['prompt']} for item in all_preference_data])\n",
    "            datasets['grpo'] = Dataset.from_list([{'prompt': item['prompt']} for item in all_preference_data])\n",
    "            print(f\"  ‚úÖ Created datasets - DPO: {len(datasets['dpo'])}, PPO: {len(datasets['ppo'])}, GRPO: {len(datasets['grpo'])}\")\n",
    "        return datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example: Prepare and Inspect Datasets\n",
    "\n",
    "Let's use the manager to prepare the datasets and inspect their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß STEP 3: PREPARING ALIGNMENT DATASETS\n",
      "Converting data for PPO, DPO, and GRPO training...\n",
      "  üìä Processing comprehensive_preferences (5 samples)...\n",
      "  üìä Processing hh_rlhf (10 samples)...\n",
      "  ‚úÖ Created datasets - DPO: 5, PPO: 5, GRPO: 5\n",
      "dpo: 5 samples\n",
      "ppo: 5 samples\n",
      "grpo: 5 samples\n"
     ]
    }
   ],
   "source": [
    "manager = AlignmentComparisonManager()\n",
    "# Assume datasets_info is already loaded from previous step\n",
    "datasets = manager.prepare_datasets(datasets_info)\n",
    "for name, ds in datasets.items():\n",
    "    print(f\"{name}: {len(ds)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Setting Up the Model for Alignment Training\n",
    "\n",
    "In this step, we'll set up the language model and tokenizer for alignment training.\n",
    "\n",
    "We'll use a small model (like GPT-2 or DistilGPT-2) for demonstration, and configure it for parameter-efficient fine-tuning (LoRA).\n",
    "\n",
    "---\n",
    "\n",
    "## Function: `setup_models`\n",
    "\n",
    "This function loads and configures the model and tokenizer for alignment training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "class AlignmentComparisonManager:\n",
    "    def __init__(self):\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "    def setup_models(self):\n",
    "        \"\"\"Setup models for alignment training\"\"\"\n",
    "        print(\"\\nüîß STEP 4: SETTING UP MODELS\")\n",
    "        print(\"Loading and configuring models for alignment training...\")\n",
    "        model_options = ['distilgpt2', 'gpt2']\n",
    "        for model_name in model_options:\n",
    "            try:\n",
    "                print(f'üîÑ Loading {model_name}...')\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                if tokenizer.pad_token is None:\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    device_map=None,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                model = model.to(self.device)\n",
    "                # Setup LoRA\n",
    "                lora_config = LoraConfig(\n",
    "                    task_type=TaskType.CAUSAL_LM,\n",
    "                    r=8,\n",
    "                    lora_alpha=16,\n",
    "                    lora_dropout=0.1,\n",
    "                    target_modules=['c_attn'],\n",
    "                    bias='none'\n",
    "                )\n",
    "                model = get_peft_model(model, lora_config)\n",
    "                print(f'‚úÖ Successfully loaded {model_name}')\n",
    "                return {'model': model, 'tokenizer': tokenizer, 'name': model_name}\n",
    "            except Exception as e:\n",
    "                print(f'‚ùå Failed to load {model_name}: {e}')\n",
    "                continue\n",
    "        raise RuntimeError('Failed to load any model')\n",
    "    \n",
    "        # üîß STEP 4: SETTING UP MODELS\"\n",
    "        print(\"Loading and configuring models for alignment training...\")\n",
    "        model_options = ['distilgpt2', 'gpt2']\n",
    "        for model_name in model_options:\n",
    "            try:\n",
    "                print(f'üîÑ Loading {model_name}...')\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                if tokenizer.pad_token is None:\n",
    "                    tokenizer.pad_token = tokenizer.eos_token\n",
    "                    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "                model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    torch_dtype=torch.float32,\n",
    "                    device_map=None,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                model = model.to(self.device)\n",
    "                # Setup LoRA\n",
    "                lora_config = LoraConfig(\n",
    "                    task_type=TaskType.CAUSAL_LM,\n",
    "                    r=8,\n",
    "                    lora_alpha=16,\n",
    "                    lora_dropout=0.1,\n",
    "                    target_modules=['c_attn'],\n",
    "                    bias='none'\n",
    "                )\n",
    "                model = get_peft_model(model, lora_config)\n",
    "                print(f'‚úÖ Successfully loaded {model_name}')\n",
    "                return {'model': model, 'tokenizer': tokenizer, 'name': model_name}\n",
    "            except Exception as e:\n",
    "                print(f'‚ùå Failed to load {model_name}: {e}')\n",
    "                continue\n",
    "        raise RuntimeError('Failed to load any model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example: Setup the Model\n",
    "\n",
    "Let's use the manager to set up the model and tokenizer for alignment training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß STEP 4: SETTING UP MODELS\n",
      "Loading and configuring models for alignment training...\n",
      "üîÑ Loading distilgpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "‚úÖ Successfully loaded distilgpt2\n",
      "Loaded model: distilgpt2\n"
     ]
    }
   ],
   "source": [
    "   manager = AlignmentComparisonManager()\n",
    "   model_info = manager.setup_models()\n",
    "   print(f\"Loaded model: {model_info['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Preference Optimization (DPO) Explained:\n",
    "\n",
    "DPO optimizes the model to prefer chosen responses over rejected ones using this loss function:\n",
    "\n",
    "```\n",
    "L_DPO = -log(œÉ(Œ≤ * log(œÄ_Œ∏(y_w|x)/œÄ_ref(y_w|x)) - Œ≤ * log(œÄ_Œ∏(y_l|x)/œÄ_ref(y_l|x))))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `y_w` = chosen response\n",
    "- `y_l` = rejected response  \n",
    "- `Œ≤` = beta parameter (controls preference strength)\n",
    "- `œÄ_Œ∏` = current model\n",
    "- `œÄ_ref` = reference model\n",
    "\n",
    "**Key Parameters:**\n",
    "- **Beta (Œ≤)**: Higher values = stronger preference enforcement\n",
    "- **Learning Rate**: Lower for stable alignment\n",
    "- **Batch Size**: Small for memory efficiency\n",
    "- **Max Length**: Shorter sequences for stability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Training (Classic RLHF)\n",
    "\n",
    "**What is PPO?**\n",
    "\n",
    "PPO (Proximal Policy Optimization) is the classic reinforcement learning approach used in the original RLHF pipeline (ChatGPT). Unlike DPO, PPO requires a separate reward model and uses reinforcement learning to optimize the policy.\n",
    "\n",
    "**PPO vs DPO Comparison:**\n",
    "\n",
    "| Aspect | PPO/RLHF | DPO |\n",
    "|--------|----------|-----|\n",
    "| **Complexity** | High (needs reward model + RL) | Low (direct optimization) |\n",
    "| **Stability** | Can be unstable | More stable |\n",
    "| **Speed** | Slower (multiple models) | Faster (single model) |\n",
    "| **Memory** | Higher (policy + value + reward models) | Lower (policy + reference) |\n",
    "| **Data Requirements** | Preference pairs ‚Üí Reward model ‚Üí RL | Direct preference pairs |\n",
    "\n",
    "**PPO Process:**\n",
    "1. **Collect Preferences**: Human feedback on model outputs\n",
    "2. **Train Reward Model**: Predict human preferences \n",
    "3. **RL Training**: Use PPO to maximize reward while staying close to reference model\n",
    "4. **Iterate**: Repeat process for continuous improvement\n",
    "\n",
    "**When to Use PPO:**\n",
    "- Need explicit reward signals\n",
    "- Have large-scale preference data\n",
    "- Want fine-grained control over reward modeling\n",
    "- Building complex multi-objective systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Cutting-Edge: GRPO Training\n",
    "\n",
    "**What is GRPO?**\n",
    "\n",
    "GRPO (Generalized Reward Preference Optimization) is the newest alignment technique that combines the best of both PPO and DPO. It's more flexible than DPO and more stable than PPO.\n",
    "\n",
    "**GRPO Advantages:**\n",
    "- **Flexible Feedback**: Handles pairs, rankings, numeric scores, and mixed feedback types\n",
    "- **Built-in Reward Modeling**: Learns reward model and policy simultaneously\n",
    "- **Sample Efficient**: Requires less data than traditional RLHF\n",
    "- **Stable Training**: More robust than PPO, more flexible than DPO\n",
    "- **Multi-Objective**: Can optimize for multiple alignment goals\n",
    "\n",
    "**GRPO vs Other Methods:**\n",
    "\n",
    "| Feature | PPO | DPO | GRPO |\n",
    "|---------|-----|-----|------|\n",
    "| **Feedback Types** | Scores only | Pairs only | Any type |\n",
    "| **Reward Model** | Separate training | Not needed | Built-in |\n",
    "| **Stability** | Moderate | High | High |\n",
    "| **Flexibility** | Low | Moderate | High |\n",
    "| **Implementation** | Complex | Simple | Moderate |\n",
    "\n",
    "**When to Use GRPO:**\n",
    "- Have mixed types of feedback data\n",
    "- Need multi-objective alignment\n",
    "- Want cutting-edge performance\n",
    "- Building production alignment systems\n",
    "\n",
    "**GRPO Process:**\n",
    "1. **Unified Data Processing**: Handle multiple feedback formats\n",
    "2. **Joint Training**: Simultaneously learn reward model and policy\n",
    "3. **Adaptive Optimization**: Adjust to different feedback types\n",
    "4. **Multi-Objective Balancing**: Optimize for multiple alignment goals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Demonstrating Alignment Methods (PPO, DPO, GRPO)\n",
    "\n",
    "**Why Evaluate Alignment?**\n",
    "\n",
    "- **Measure Improvement**: Compare before/after responses\n",
    "- **Quality Assessment**: Ensure alignment actually helps\n",
    "- **Safety Check**: Verify no harmful behavior\n",
    "- **Task Performance**: Maintain task-specific capabilities\n",
    "\n",
    "**Evaluation Methods:**\n",
    "- **Qualitative**: Human evaluation of response quality\n",
    "- **Quantitative**: Automated metrics and benchmarks\n",
    "- **Comparative**: Before vs after alignment\n",
    "- **Safety**: Red-teaming and adversarial testing\n",
    "\n",
    "In this step, we'll demonstrate the three major alignment methods: PPO, DPO, and GRPO.\n",
    "\n",
    "We'll run a simplified training loop for each method and observe the results.\n",
    "\n",
    "---\n",
    "\n",
    "## Function: `demonstrate_alignment_methods`\n",
    "\n",
    "This function runs a demonstration of PPO, DPO, and GRPO training using the prepared datasets and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class AlignmentComparisonManager:\n",
    "    def _prepare_datasets_for_training(self, datasets, tokenizer):\n",
    "        \"\"\"Prepare datasets with proper tokenization for training\"\"\"\n",
    "        prepared_datasets = {}\n",
    "        # Prepare PPO dataset\n",
    "        if \"ppo\" in datasets:\n",
    "            ppo_data = []\n",
    "            for item in datasets[\"ppo\"]:\n",
    "                tokenized = tokenizer(\n",
    "                    item[\"query\"], \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True, \n",
    "                    truncation=True,\n",
    "                    max_length=128\n",
    "                )\n",
    "                ppo_data.append({\n",
    "                    \"input_ids\": tokenized[\"input_ids\"].squeeze(),\n",
    "                    \"attention_mask\": tokenized[\"attention_mask\"].squeeze(),\n",
    "                    \"query\": item[\"query\"]\n",
    "                })\n",
    "            if ppo_data:\n",
    "                prepared_datasets[\"ppo\"] = ppo_data\n",
    "        # Prepare DPO dataset  \n",
    "        if \"dpo\" in datasets:\n",
    "            dpo_data = []\n",
    "            for item in datasets[\"dpo\"]:\n",
    "                if all(key in item for key in [\"prompt\", \"chosen\", \"rejected\"]):\n",
    "                    dpo_data.append({\n",
    "                        \"prompt\": str(item[\"prompt\"]),\n",
    "                        \"chosen\": str(item[\"chosen\"]),\n",
    "                        \"rejected\": str(item[\"rejected\"])\n",
    "                    })\n",
    "            if dpo_data:\n",
    "                prepared_datasets[\"dpo\"] = dpo_data\n",
    "        # Prepare GRPO dataset\n",
    "        if \"grpo\" in datasets:\n",
    "            grpo_data = []\n",
    "            for item in datasets[\"grpo\"]:\n",
    "                if \"prompt\" in item:\n",
    "                    grpo_data.append({\n",
    "                        \"prompt\": str(item[\"prompt\"])\n",
    "                    })\n",
    "            if grpo_data:\n",
    "                prepared_datasets[\"grpo\"] = grpo_data\n",
    "        return prepared_datasets\n",
    "\n",
    "    def _test_ppo_training(self, model_info, dataset):\n",
    "        if not dataset or len(dataset) == 0:\n",
    "            return {\"status\": \"no_data\", \"message\": \"No dataset available\"}\n",
    "        model = model_info[\"model\"]\n",
    "        tokenizer = model_info[\"tokenizer\"]\n",
    "        losses = []\n",
    "        for step in range(min(3, len(dataset))):\n",
    "            query = dataset[step][\"query\"]\n",
    "            inputs = tokenizer(\n",
    "                f\"Query: {query}\\nResponse:\",\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            ).to(model.device)\n",
    "            model.train()\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        grad_norm = torch.norm(param.grad.data)\n",
    "                        if grad_norm > 1.0:\n",
    "                            param.grad.data = param.grad.data / grad_norm\n",
    "                        param.data -= 1e-5 * param.grad.data\n",
    "                        param.grad.zero_()\n",
    "            losses.append(loss.item())\n",
    "        return {\n",
    "            \"status\": \"trained_successfully\",\n",
    "            \"losses\": losses,\n",
    "            \"final_loss\": losses[-1] if losses else 0,\n",
    "            \"steps\": len(losses)\n",
    "        }\n",
    "\n",
    "    def _test_dpo_training(self, model_info, dataset):\n",
    "        if not dataset or len(dataset) == 0:\n",
    "            return {\"status\": \"no_data\", \"message\": \"No dataset available\"}\n",
    "        model = model_info[\"model\"]\n",
    "        tokenizer = model_info[\"tokenizer\"]\n",
    "        losses = []\n",
    "        for step in range(min(3, len(dataset))):\n",
    "            data = dataset[step]\n",
    "            prompt = data[\"prompt\"]\n",
    "            chosen = data[\"chosen\"]\n",
    "            rejected = data[\"rejected\"]\n",
    "            chosen_text = f\"{prompt}\\n{chosen}\"\n",
    "            rejected_text = f\"{prompt}\\n{rejected}\"\n",
    "            chosen_inputs = tokenizer(\n",
    "                chosen_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            ).to(model.device)\n",
    "            rejected_inputs = tokenizer(\n",
    "                rejected_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            ).to(model.device)\n",
    "            model.train()\n",
    "            chosen_outputs = model(**chosen_inputs, labels=chosen_inputs[\"input_ids\"])\n",
    "            rejected_outputs = model(**rejected_inputs, labels=rejected_inputs[\"input_ids\"])\n",
    "            chosen_loss = chosen_outputs.loss\n",
    "            rejected_loss = rejected_outputs.loss\n",
    "            beta = 0.1\n",
    "            dpo_loss = -torch.log(torch.sigmoid(beta * (rejected_loss - chosen_loss)))\n",
    "            dpo_loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        param.data -= 5e-6 * param.grad.data\n",
    "                        param.grad.zero_()\n",
    "            losses.append(dpo_loss.item())\n",
    "        return {\n",
    "            \"status\": \"trained_successfully\",\n",
    "            \"losses\": losses,\n",
    "            \"final_loss\": losses[-1] if losses else 0,\n",
    "            \"steps\": len(losses)\n",
    "        }\n",
    "\n",
    "    def _test_grpo_training(self, model_info, dataset):\n",
    "        if not dataset or len(dataset) == 0:\n",
    "            return {\"status\": \"no_data\", \"message\": \"No dataset available\"}\n",
    "        model = model_info[\"model\"]\n",
    "        tokenizer = model_info[\"tokenizer\"]\n",
    "        losses = []\n",
    "        for step in range(min(3, len(dataset))):\n",
    "            prompt = dataset[step][\"prompt\"]\n",
    "            inputs = tokenizer(\n",
    "                f\"Question: {prompt}\\nAnswer:\",\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128\n",
    "            ).to(model.device)\n",
    "            with torch.no_grad():\n",
    "                group_responses = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=32,\n",
    "                    num_return_sequences=4,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.8,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            model.train()\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            base_loss = outputs.loss\n",
    "            group_losses = []\n",
    "            for response in group_responses:\n",
    "                response_inputs = {\"input_ids\": response.unsqueeze(0), \"attention_mask\": torch.ones_like(response.unsqueeze(0))}\n",
    "                try:\n",
    "                    response_output = model(**response_inputs, labels=response.unsqueeze(0))\n",
    "                    group_losses.append(response_output.loss.item())\n",
    "                except:\n",
    "                    group_losses.append(base_loss.item())\n",
    "            mean_group_loss = np.mean(group_losses)\n",
    "            std_group_loss = np.std(group_losses) + 1e-8\n",
    "            relative_advantage = (base_loss.item() - mean_group_loss) / std_group_loss\n",
    "            grpo_loss = base_loss * (1 + 0.1 * relative_advantage)\n",
    "            grpo_loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        param.data -= 1e-5 * param.grad.data\n",
    "                        param.grad.zero_()\n",
    "            losses.append(grpo_loss.item())\n",
    "        return {\n",
    "            \"status\": \"trained_successfully\",\n",
    "            \"losses\": losses,\n",
    "            \"final_loss\": losses[-1] if losses else 0,\n",
    "            \"steps\": len(losses),\n",
    "            \"group_size\": 4\n",
    "        }\n",
    "\n",
    "    def demonstrate_alignment_methods(self, model_info, datasets):\n",
    "        \"\"\"Demonstrate all alignment methods (PPO, DPO, GRPO)\"\"\"\n",
    "        print(\"\\nüéØ STEP 5: DEMONSTRATING ALIGNMENT METHODS\")\n",
    "        print(\"Implementing PPO, DPO, and GRPO concepts...\")\n",
    "        print(\"‚ö†Ô∏è  NOTE: This includes ACTUAL TRAINING - models will be updated\")\n",
    "        print(\"‚ö†Ô∏è  Training is limited to demo purposes (few epochs/steps)\")\n",
    "        prepared_datasets = self._prepare_datasets_for_training(datasets, model_info['tokenizer'])\n",
    "        results = {}\n",
    "        print(\"\\nüü° PPO (Proximal Policy Optimization):\")\n",
    "        try:\n",
    "            ppo_results = self._test_ppo_training(model_info, prepared_datasets.get('ppo'))\n",
    "            results['ppo'] = ppo_results\n",
    "        except Exception as e:\n",
    "            print(f'  ‚ùå PPO training error: {e}')\n",
    "            results['ppo'] = {'status': 'error', 'error': str(e)}\n",
    "        print(\"\\nüü¢ DPO (Direct Preference Optimization):\")\n",
    "        try:\n",
    "            dpo_results = self._test_dpo_training(model_info, prepared_datasets.get('dpo'))\n",
    "            results['dpo'] = dpo_results\n",
    "        except Exception as e:\n",
    "            print(f'  ‚ùå DPO training error: {e}')\n",
    "            results['dpo'] = {'status': 'error', 'error': str(e)}\n",
    "        print(\"\\nüü£ GRPO (Group Relative Policy Optimization):\")\n",
    "        try:\n",
    "            grpo_results = self._test_grpo_training(model_info, prepared_datasets.get('grpo'))\n",
    "            results['grpo'] = grpo_results\n",
    "        except Exception as e:\n",
    "            print(f'  ‚ùå GRPO training error: {e}')\n",
    "            results['grpo'] = {'status': 'error', 'error': str(e)}\n",
    "        print(\"\\n‚úÖ All alignment methods training completed!\")\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example: Run the Alignment Methods Demonstration\n",
    "\n",
    "Let's use the manager to run the demonstration and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ STEP 5: DEMONSTRATING ALIGNMENT METHODS\n",
      "Implementing PPO, DPO, and GRPO concepts...\n",
      "‚ö†Ô∏è  NOTE: This includes ACTUAL TRAINING - models will be updated\n",
      "‚ö†Ô∏è  Training is limited to demo purposes (few epochs/steps)\n",
      "\n",
      "üü° PPO (Proximal Policy Optimization):\n",
      "\n",
      "üü¢ DPO (Direct Preference Optimization):\n",
      "\n",
      "üü£ GRPO (Group Relative Policy Optimization):\n",
      "\n",
      "‚úÖ All alignment methods training completed!\n",
      "{'ppo': {'status': 'trained_successfully', 'losses': [3.942887306213379, 3.4061694145202637, 3.471867322921753], 'final_loss': 3.471867322921753, 'steps': 3}, 'dpo': {'status': 'trained_successfully', 'losses': [0.7097368836402893, 0.7240135669708252, 0.7008044123649597], 'final_loss': 0.7008044123649597, 'steps': 3}, 'grpo': {'status': 'trained_successfully', 'losses': [4.241724014282227, 4.295168399810791, 5.627329349517822], 'final_loss': 5.627329349517822, 'steps': 3, 'group_size': 4}}\n"
     ]
    }
   ],
   "source": [
    "   manager = AlignmentComparisonManager()\n",
    "   results = manager.demonstrate_alignment_methods(model_info, datasets)\n",
    "   print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Evaluation and Comparison of Alignment Methods\n",
    "\n",
    "In this step, we'll evaluate and compare the performance of the different alignment methods (Base, PPO, DPO, GRPO) using a simple quality scoring heuristic.\n",
    "\n",
    "We'll use simulated responses for demonstration, but you can adapt this to use real model outputs if available.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä STEP 7: EVALUATING ALIGNMENT METHODS\n",
      "Comparing method performance across domains...\n",
      "  üìù BASE: Average score 0.240\n",
      "  üìù PPO: Average score 0.540\n",
      "  üìù DPO: Average score 0.520\n",
      "  üìù GRPO: Average score 0.540\n",
      "\n",
      "Summary of evaluation:\n",
      "BASE: Avg Score = 0.240, Responses = ['Make a list and work through it based on deadlines.', 'Machine learning is when computers learn from examples.', 'Turn off affected systems and call security team.', 'Look at code and check for bugs and standards.', 'Talk to them about performance and set expectations.']\n",
      "PPO: Avg Score = 0.540, Responses = ['Use systematic prioritization: assess true deadlines, evaluate impact, consider dependencies, use frameworks like Eisenhower Matrix, communicate with stakeholders about realistic timelines.', 'Machine learning is like teaching a computer to recognize patterns! Show it thousands of examples, and it learns to make good guesses on new information.', 'Execute structured incident response: immediate containment, activate response team, assess scope, preserve evidence, notify stakeholders, implement remediation.', 'Comprehensive reviews focus on functionality, security, performance, maintainability, standards adherence, test coverage, and knowledge sharing opportunities.', 'Address underperformance with structured approach: private discussion, understand causes, collaborate on improvement goals, provide support, regular check-ins.']\n",
      "DPO: Avg Score = 0.520, Responses = [\"Use ICE framework (Impact, Confidence, Ease) to rank tasks, challenge 'urgent' assumptions, communicate constraints transparently, focus on business value.\", 'Think of it like teaching pattern recognition to a computer! Just like you learned faces by seeing examples, computers learn from data patterns.', 'Follow NIST framework: Prepare, Detect/Analyze, Contain/Eradicate/Recover, Post-incident review. Focus on business continuity and evidence preservation.', 'Balance thoroughness with efficiency: focus on logic/security/maintainability, use automation for basics, provide specific actionable feedback.', 'Start with curiosity: understand barriers through open questions, collaborate on improvement plan, provide resources, regular supportive check-ins.']\n",
      "GRPO: Avg Score = 0.540, Responses = ['Structured system: list tasks with effort estimates, score impact√óurgency/effort, identify dependencies, communicate timelines, batch similar tasks, daily priority review.', 'Machine learning is like a super-smart friend practicing games! Show 1000 animal pictures with answers, they get amazing at guessing new animals by recognizing patterns!', 'Phased response: Phase 1-Containment, Phase 2-Investigation, Phase 3-Communication, Phase 4-Recovery, Phase 5-Post-mortem with lessons learned.', 'Structured process: automated checks first, review architecture/logic/performance, verify error handling/tests/docs, provide specific feedback with examples.', 'Systematic approach: data gathering, root cause analysis, SMART goal setting, resource allocation, regular monitoring, plan adjustment, achievement recognition.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AlignmentComparisonManager:\n",
    "    def _calculate_quality_score(self, response: str) -> float:\n",
    "        \"\"\"Calculate response quality score (simple heuristic)\"\"\"\n",
    "        score = 0.0\n",
    "        word_count = len(response.split())\n",
    "        if 20 <= word_count <= 150:\n",
    "            score += 0.3\n",
    "        if any(indicator in response.lower() for indicator in [\"1)\", \"2)\", \"first\", \"then\", \":\"]):\n",
    "            score += 0.3\n",
    "        if any(word in response.lower() for word in [\"example\", \"specific\", \"such as\"]):\n",
    "            score += 0.2\n",
    "        if not any(word in response.lower() for word in [\"gonna\", \"wanna\", \"kinda\"]):\n",
    "            score += 0.2\n",
    "        return min(score, 1.0)\n",
    "\n",
    "    def _get_simulated_responses(self):\n",
    "        \"\"\"Get simulated responses as fallback (for demonstration)\"\"\"\n",
    "        return {\n",
    "            \"base\": [\n",
    "                \"Make a list and work through it based on deadlines.\",\n",
    "                \"Machine learning is when computers learn from examples.\",\n",
    "                \"Turn off affected systems and call security team.\",\n",
    "                \"Look at code and check for bugs and standards.\",\n",
    "                \"Talk to them about performance and set expectations.\"\n",
    "            ],\n",
    "            \"ppo\": [\n",
    "                \"Use systematic prioritization: assess true deadlines, evaluate impact, consider dependencies, use frameworks like Eisenhower Matrix, communicate with stakeholders about realistic timelines.\",\n",
    "                \"Machine learning is like teaching a computer to recognize patterns! Show it thousands of examples, and it learns to make good guesses on new information.\",\n",
    "                \"Execute structured incident response: immediate containment, activate response team, assess scope, preserve evidence, notify stakeholders, implement remediation.\",\n",
    "                \"Comprehensive reviews focus on functionality, security, performance, maintainability, standards adherence, test coverage, and knowledge sharing opportunities.\",\n",
    "                \"Address underperformance with structured approach: private discussion, understand causes, collaborate on improvement goals, provide support, regular check-ins.\"\n",
    "            ],\n",
    "            \"dpo\": [\n",
    "                \"Use ICE framework (Impact, Confidence, Ease) to rank tasks, challenge 'urgent' assumptions, communicate constraints transparently, focus on business value.\",\n",
    "                \"Think of it like teaching pattern recognition to a computer! Just like you learned faces by seeing examples, computers learn from data patterns.\",\n",
    "                \"Follow NIST framework: Prepare, Detect/Analyze, Contain/Eradicate/Recover, Post-incident review. Focus on business continuity and evidence preservation.\",\n",
    "                \"Balance thoroughness with efficiency: focus on logic/security/maintainability, use automation for basics, provide specific actionable feedback.\",\n",
    "                \"Start with curiosity: understand barriers through open questions, collaborate on improvement plan, provide resources, regular supportive check-ins.\"\n",
    "            ],\n",
    "            \"grpo\": [\n",
    "                \"Structured system: list tasks with effort estimates, score impact√óurgency/effort, identify dependencies, communicate timelines, batch similar tasks, daily priority review.\",\n",
    "                \"Machine learning is like a super-smart friend practicing games! Show 1000 animal pictures with answers, they get amazing at guessing new animals by recognizing patterns!\",\n",
    "                \"Phased response: Phase 1-Containment, Phase 2-Investigation, Phase 3-Communication, Phase 4-Recovery, Phase 5-Post-mortem with lessons learned.\",\n",
    "                \"Structured process: automated checks first, review architecture/logic/performance, verify error handling/tests/docs, provide specific feedback with examples.\",\n",
    "                \"Systematic approach: data gathering, root cause analysis, SMART goal setting, resource allocation, regular monitoring, plan adjustment, achievement recognition.\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def evaluate_methods(self):\n",
    "        \"\"\"Step 7: Evaluate and compare alignment methods\"\"\"\n",
    "        print(\"\\nüìä STEP 7: EVALUATING ALIGNMENT METHODS\")\n",
    "        print(\"Comparing method performance across domains...\")\n",
    "\n",
    "        evaluation_prompts = [\n",
    "            \"How do you prioritize tasks when everything seems urgent?\",\n",
    "            \"Explain machine learning to a 5-year-old.\",\n",
    "            \"How do you handle a security breach?\",\n",
    "            \"What's your approach to code reviews?\",\n",
    "            \"How do you motivate underperforming team members?\"\n",
    "        ]\n",
    "\n",
    "        # For demonstration, use simulated responses\n",
    "        method_scores = {}\n",
    "        method_responses = self._get_simulated_responses()\n",
    "\n",
    "        # Calculate quality scores for each method\n",
    "        for method, responses in method_responses.items():\n",
    "            scores = []\n",
    "            for response in responses:\n",
    "                score = self._calculate_quality_score(response)\n",
    "                scores.append(score)\n",
    "            method_scores[method] = {\n",
    "                \"average_score\": np.mean(scores),\n",
    "                \"std_score\": np.std(scores),\n",
    "                \"responses\": responses\n",
    "            }\n",
    "            print(f\"  üìù {method.upper()}: Average score {method_scores[method]['average_score']:.3f}\")\n",
    "\n",
    "        return {\n",
    "            \"method_scores\": method_scores,\n",
    "            \"evaluation_prompts\": evaluation_prompts\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "manager = AlignmentComparisonManager()\n",
    "results = manager.evaluate_methods()\n",
    "print(\"\\nSummary of evaluation:\")\n",
    "for method, info in results[\"method_scores\"].items():\n",
    "    print(f\"{method.upper()}: Avg Score = {info['average_score']:.3f}, Responses = {info['responses']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Generate a Comprehensive Alignment Report\n",
    "\n",
    "In this step, we'll generate a comprehensive report summarizing the evaluation results and providing recommendations for each alignment method (Base, PPO, DPO, GRPO).\n",
    "\n",
    "The report will include:\n",
    "- A summary of the best method and its score\n",
    "- Key characteristics of each method\n",
    "- Recommendations for different use cases\n",
    "- Key insights from the evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã STEP 8: GENERATING COMPREHENSIVE REPORT\n",
      "Creating detailed analysis and recommendations...\n",
      "‚úÖ Comprehensive report generated!\n",
      "{\n",
      "  \"summary\": {\n",
      "    \"best_method\": \"ppo\",\n",
      "    \"best_score\": 0.54,\n",
      "    \"methods_evaluated\": [\n",
      "      \"base\",\n",
      "      \"ppo\",\n",
      "      \"dpo\",\n",
      "      \"grpo\"\n",
      "    ]\n",
      "  },\n",
      "  \"method_characteristics\": {\n",
      "    \"base\": {\n",
      "      \"complexity\": \"Low\",\n",
      "      \"training_time\": \"None\",\n",
      "      \"memory_usage\": \"Low\",\n",
      "      \"strengths\": [\n",
      "        \"Simple\",\n",
      "        \"Fast\",\n",
      "        \"No training\"\n",
      "      ],\n",
      "      \"weaknesses\": [\n",
      "        \"No alignment\",\n",
      "        \"Inconsistent\",\n",
      "        \"No safety\"\n",
      "      ]\n",
      "    },\n",
      "    \"ppo\": {\n",
      "      \"complexity\": \"High\",\n",
      "      \"training_time\": \"Long\",\n",
      "      \"memory_usage\": \"Very High\",\n",
      "      \"strengths\": [\n",
      "        \"Proven method\",\n",
      "        \"Fine control\",\n",
      "        \"Stable\"\n",
      "      ],\n",
      "      \"weaknesses\": [\n",
      "        \"4 models needed\",\n",
      "        \"Complex\",\n",
      "        \"High cost\"\n",
      "      ]\n",
      "    },\n",
      "    \"dpo\": {\n",
      "      \"complexity\": \"Medium\",\n",
      "      \"training_time\": \"Medium\",\n",
      "      \"memory_usage\": \"Medium\",\n",
      "      \"strengths\": [\n",
      "        \"No reward model\",\n",
      "        \"Simpler\",\n",
      "        \"Good results\"\n",
      "      ],\n",
      "      \"weaknesses\": [\n",
      "        \"Limited to preferences\",\n",
      "        \"Beta tuning\"\n",
      "      ]\n",
      "    },\n",
      "    \"grpo\": {\n",
      "      \"complexity\": \"Medium-High\",\n",
      "      \"training_time\": \"Medium-Long\",\n",
      "      \"memory_usage\": \"High\",\n",
      "      \"strengths\": [\n",
      "        \"Sample efficient\",\n",
      "        \"Flexible\",\n",
      "        \"Latest method\"\n",
      "      ],\n",
      "      \"weaknesses\": [\n",
      "        \"Multiple generations\",\n",
      "        \"Newer method\"\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"recommendations\": {\n",
      "    \"beginners\": \"Start with DPO - best balance of simplicity and performance\",\n",
      "    \"production\": \"DPO for most cases, PPO for safety-critical applications\",\n",
      "    \"research\": \"GRPO for latest innovations and flexible experimentation\",\n",
      "    \"resources\": \"DPO offers best performance per computational cost\"\n",
      "  },\n",
      "  \"key_insights\": [\n",
      "    \"DPO provides best balance of performance and simplicity\",\n",
      "    \"GRPO shows promise for complex reasoning tasks\",\n",
      "    \"PPO remains essential for safety-critical applications\",\n",
      "    \"Base models lack consistency for production use\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class AlignmentComparisonManager:\n",
    "    # ... (include previous methods here, or assume this is added to your existing class)\n",
    "\n",
    "    def generate_report(self, evaluation_results):\n",
    "        \"\"\"Step 8: Generate comprehensive report\"\"\"\n",
    "        print(\"\\nüìã STEP 8: GENERATING COMPREHENSIVE REPORT\")\n",
    "        print(\"Creating detailed analysis and recommendations...\")\n",
    "\n",
    "        method_characteristics = {\n",
    "            \"base\": {\n",
    "                \"complexity\": \"Low\",\n",
    "                \"training_time\": \"None\", \n",
    "                \"memory_usage\": \"Low\",\n",
    "                \"strengths\": [\"Simple\", \"Fast\", \"No training\"],\n",
    "                \"weaknesses\": [\"No alignment\", \"Inconsistent\", \"No safety\"]\n",
    "            },\n",
    "            \"ppo\": {\n",
    "                \"complexity\": \"High\",\n",
    "                \"training_time\": \"Long\",\n",
    "                \"memory_usage\": \"Very High\", \n",
    "                \"strengths\": [\"Proven method\", \"Fine control\", \"Stable\"],\n",
    "                \"weaknesses\": [\"4 models needed\", \"Complex\", \"High cost\"]\n",
    "            },\n",
    "            \"dpo\": {\n",
    "                \"complexity\": \"Medium\",\n",
    "                \"training_time\": \"Medium\",\n",
    "                \"memory_usage\": \"Medium\",\n",
    "                \"strengths\": [\"No reward model\", \"Simpler\", \"Good results\"],\n",
    "                \"weaknesses\": [\"Limited to preferences\", \"Beta tuning\"]\n",
    "            },\n",
    "            \"grpo\": {\n",
    "                \"complexity\": \"Medium-High\", \n",
    "                \"training_time\": \"Medium-Long\",\n",
    "                \"memory_usage\": \"High\",\n",
    "                \"strengths\": [\"Sample efficient\", \"Flexible\", \"Latest method\"],\n",
    "                \"weaknesses\": [\"Multiple generations\", \"Newer method\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        recommendations = {\n",
    "            \"beginners\": \"Start with DPO - best balance of simplicity and performance\",\n",
    "            \"production\": \"DPO for most cases, PPO for safety-critical applications\", \n",
    "            \"research\": \"GRPO for latest innovations and flexible experimentation\",\n",
    "            \"resources\": \"DPO offers best performance per computational cost\"\n",
    "        }\n",
    "\n",
    "        if evaluation_results and \"method_scores\" in evaluation_results:\n",
    "            best_method = max(\n",
    "                evaluation_results[\"method_scores\"].keys(), \n",
    "                key=lambda x: evaluation_results[\"method_scores\"][x][\"average_score\"]\n",
    "            )\n",
    "            best_score = evaluation_results[\"method_scores\"][best_method][\"average_score\"]\n",
    "        else:\n",
    "            best_method = \"dpo\"\n",
    "            best_score = 0.85\n",
    "\n",
    "        report = {\n",
    "            \"summary\": {\n",
    "                \"best_method\": best_method,\n",
    "                \"best_score\": best_score,\n",
    "                \"methods_evaluated\": [\"base\", \"ppo\", \"dpo\", \"grpo\"]\n",
    "            },\n",
    "            \"method_characteristics\": method_characteristics,\n",
    "            \"recommendations\": recommendations,\n",
    "            \"key_insights\": [\n",
    "                \"DPO provides best balance of performance and simplicity\",\n",
    "                \"GRPO shows promise for complex reasoning tasks\",\n",
    "                \"PPO remains essential for safety-critical applications\",\n",
    "                \"Base models lack consistency for production use\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # Optionally save to file\n",
    "        with open(\"alignment_comparison_report.json\", \"w\") as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "\n",
    "        print(\"‚úÖ Comprehensive report generated!\")\n",
    "        print(json.dumps(report, indent=2))\n",
    "        return report\n",
    "\n",
    "# Example usage:\n",
    "# (Assume you have already run evaluation and have `results`)\n",
    "manager = AlignmentComparisonManager()\n",
    "report = manager.generate_report(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Save All Results\n",
    "\n",
    "In this final step, we'll save all the key results and artifacts from our alignment comparison workflow.  \n",
    "This includes:\n",
    "- The comprehensive report\n",
    "- Evaluation results\n",
    "- Any collected annotations (if available)\n",
    "- A summary of the learning outcomes\n",
    "\n",
    "Saving these results ensures reproducibility and makes it easy to share or revisit your findings later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ STEP 9: SAVING RESULTS\n",
      "Saving all artifacts and analysis...\n",
      "‚úÖ All results saved!\n",
      "{\n",
      "  \"class\": \"Class 7 - Alignment Method Comparison\",\n",
      "  \"methods\": [\n",
      "    \"PPO\",\n",
      "    \"DPO\",\n",
      "    \"GRPO\"\n",
      "  ],\n",
      "  \"base_model\": \"gpt2\",\n",
      "  \"annotations_collected\": 0,\n",
      "  \"best_method\": \"ppo\",\n",
      "  \"best_score\": 0.54,\n",
      "  \"artifacts\": [\n",
      "    \"alignment_comparison_report.json\",\n",
      "    \"evaluation_results.json\",\n",
      "    null\n",
      "  ],\n",
      "  \"learning_outcomes\": [\n",
      "    \"Understanding of PPO, DPO, and GRPO concepts\",\n",
      "    \"Hands-on preference data collection\",\n",
      "    \"Method comparison and evaluation\",\n",
      "    \"Interactive demonstration tools\",\n",
      "    \"Best practice recommendations\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "class AlignmentComparisonManager:\n",
    "    def __init__(self):\n",
    "        self.base_model_name = \"gpt2\"\n",
    "        self.annotations = []\n",
    "\n",
    "    def save_results(self, report, evaluation_results):\n",
    "        \"\"\"Step 9: Save all results\"\"\"\n",
    "        print(\"\\nüíæ STEP 9: SAVING RESULTS\")\n",
    "        print(\"Saving all artifacts and analysis...\")\n",
    "\n",
    "        # Save individual files\n",
    "        with open(\"alignment_comparison_report.json\", \"w\") as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "\n",
    "        with open(\"evaluation_results.json\", \"w\") as f:\n",
    "            json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "        if self.annotations:\n",
    "            with open(\"collected_annotations.json\", \"w\") as f:\n",
    "                json.dump(self.annotations, f, indent=2)\n",
    "\n",
    "        summary = {\n",
    "            \"class\": \"Class 7 - Alignment Method Comparison\",\n",
    "            \"methods\": [\"PPO\", \"DPO\", \"GRPO\"],\n",
    "            \"base_model\": self.base_model_name,\n",
    "            \"annotations_collected\": len(self.annotations),\n",
    "            \"best_method\": report[\"summary\"][\"best_method\"],\n",
    "            \"best_score\": report[\"summary\"][\"best_score\"],\n",
    "            \"artifacts\": [\n",
    "                \"alignment_comparison_report.json\",\n",
    "                \"evaluation_results.json\",\n",
    "                \"collected_annotations.json\" if self.annotations else None\n",
    "            ],\n",
    "            \"learning_outcomes\": [\n",
    "                \"Understanding of PPO, DPO, and GRPO concepts\",\n",
    "                \"Hands-on preference data collection\",\n",
    "                \"Method comparison and evaluation\",\n",
    "                \"Interactive demonstration tools\",\n",
    "                \"Best practice recommendations\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        with open(\"final_summary.json\", \"w\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "        print(\"‚úÖ All results saved!\")\n",
    "        print(json.dumps(summary, indent=2))\n",
    "        return summary\n",
    "\n",
    "# Example usage:\n",
    "# (Assume you have already run previous steps and have `report` and `results`)\n",
    "manager = AlignmentComparisonManager()\n",
    "summary = manager.save_results(report, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sft-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
