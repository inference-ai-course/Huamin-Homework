(sft-env) jianghm@Huamins-MacBook-Pro week7 % /Users/jianghm/MLE_Training/week7/sft-env/bin/python /Users/jianghm/MLE_
Training/week7/class7_1.py
âœ… TRL library loaded successfully
âœ… Gradio library loaded successfully
ğŸ¯ COMPREHENSIVE ALIGNMENT COMPARISON - CLASS 7
================================================================================
ğŸš€ Educational Implementation: PPO vs DPO vs GRPO
ğŸ“ Focus: Concepts, Implementation, and Comparison
ğŸ“š Works without Ollama - Perfect for Learning
================================================================================
ğŸ¯ ALIGNMENT COMPARISON MANAGER - CLASS 7
ğŸ“± Device: mps
ğŸ”‘ OpenAI API: âœ… Available
ğŸ¤– Base Model: gpt2
ğŸš€ Focus: PPO vs DPO vs GRPO Educational Comparison
================================================================================

ğŸ“š STEP 1: LOADING PREFERENCE DATASETS
Creating comprehensive preference data for alignment training...
  ğŸ“¥ Attempting to load real preference datasets...
  âœ… Loaded 10 samples from HH-RLHF
ğŸ“Š Total datasets available: 2

ğŸ¨ STEP 2: CREATING ANNOTATION PLATFORM
Building preference annotation interface...
ğŸ¨ Annotation platform created!

ğŸ”§ STEP 3: PREPARING ALIGNMENT DATASETS
Converting data for PPO, DPO, and GRPO training...
  ğŸ“Š Processing comprehensive_preferences (5 samples)...
  ğŸ“Š Processing hh_rlhf (10 samples)...
  âœ… Created datasets - DPO: 5, PPO: 5, GRPO: 5

ğŸ”§ STEP 4: SETTING UP MODELS
Loading and configuring models for alignment training...
ğŸ”„ Loading distilgpt2...
INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
`torch_dtype` is deprecated! Use `dtype` instead!
'NoneType' object has no attribute 'cadam32bit_grad_fp32'
trainable params: 147,456 || all params: 82,060,032 || trainable%: 0.1797
âœ… Successfully loaded distilgpt2

ğŸ¯ STEP 5: DEMONSTRATING ALIGNMENT METHODS
Implementing PPO, DPO, and GRPO concepts...
âš ï¸  NOTE: This includes ACTUAL TRAINING - models will be updated
âš ï¸  Training is limited to demo purposes (few epochs/steps)
  ğŸ”§ Preparing datasets for training...
  âœ… Prepared 3 datasets for training

ğŸŸ¡ PPO (Proximal Policy Optimization):
  ğŸ¯ Uses reward model and value function
  ğŸ“Š Formula: L_PPO = min(r_t(Î¸)A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ)A_t)
  ğŸ”„ Requires: Policy + Value + Reward + Reference models
  ğŸš€ Starting PPO training with simplified implementation...
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
    ğŸ“Š PPO Step 0: Loss = 3.9193
    ğŸ“Š PPO Step 1: Loss = 3.3954
    ğŸ“Š PPO Step 2: Loss = 3.6137
  âœ… PPO training completed successfully!
    ğŸ“Š Final loss: 3.6137
    ğŸ“Š Training steps: 3

ğŸŸ¢ DPO (Direct Preference Optimization):
  ğŸ¯ Direct preference optimization without reward model
  ğŸ“Š Formula: L_DPO = -log(Ïƒ(Î² * preference_diff))
  âœ¨ Simpler than PPO, often better results
  ğŸš€ Starting DPO training with simplified implementation...
    ğŸ“Š DPO Step 0: Loss = 0.7044 (chosen: 3.8492, rejected: 3.6260)
    ğŸ“Š DPO Step 1: Loss = 0.7297 (chosen: 3.7831, rejected: 3.0641)
    ğŸ“Š DPO Step 2: Loss = 0.7030 (chosen: 3.4556, rejected: 3.2605)
  âœ… DPO training completed successfully!
    ğŸ“Š Final loss: 0.7030
    ğŸ“Š Training steps: 3
  ğŸ’¾ DPO model saved successfully!

ğŸŸ£ GRPO (Group Relative Policy Optimization):
  ğŸ¯ Group-wise comparisons for relative ranking
  ğŸ“Š Formula: Advantage = (reward - group_mean) / group_std
  ğŸš€ More sample efficient than traditional RL
  ğŸš€ Starting GRPO training with simplified implementation...
    ğŸ“Š GRPO Step 0: Loss = 4.7591 (advantage: 3.210)
    ğŸ“Š GRPO Step 1: Loss = 4.4447 (advantage: 4.332)
    ğŸ“Š GRPO Step 2: Loss = 3.2289 (advantage: 1.043)
  âœ… GRPO training completed successfully!
    ğŸ“Š Final loss: 3.2289
    ğŸ“Š Training steps: 3
    ğŸ“Š Group size: 4
  ğŸ’¾ GRPO model saved successfully!

âœ… All alignment methods training completed!
============================================================
  â€¢ PPO: âœ… Trained (3 steps, final loss: 3.6137)
  â€¢ DPO: âœ… Trained (3 steps, final loss: 0.7030)
  â€¢ GRPO: âœ… Trained (3 steps, final loss: 3.2289)
============================================================

ğŸ“Š STEP 6: EVALUATING ALIGNMENT METHODS
Comparing method performance across domains...
  ğŸ¤– Evaluating trained models...
    ğŸ”„ Loading DPO trained model...
    âœ… Generated 5 responses from DPO
    ğŸ”„ Loading GRPO trained model...
    âœ… Generated 5 responses from GRPO
  ğŸ“ DPO: Average score 0.440
  ğŸ“ GRPO: Average score 0.480

ğŸ® STEP 7: CREATING COMPARISON DEMO
Building method comparison interface...
ğŸ® Comparison demo created!

ğŸ“‹ STEP 8: GENERATING COMPREHENSIVE REPORT
Creating detailed analysis and recommendations...
âœ… Comprehensive report generated!

================================================================================
ğŸ§ª RUNNING COMPREHENSIVE TRAINING TEST
================================================================================

ğŸ§ª COMPREHENSIVE TRAINING TEST
Testing all alignment methods with simplified implementations...
  ğŸ”§ Preparing datasets for training...
  âœ… Prepared 3 datasets for training

ğŸŸ¡ Testing PPO (Simplified Implementation):
    ğŸ“Š PPO Step 0: Loss = 3.8777
    ğŸ“Š PPO Step 1: Loss = 3.2308
    ğŸ“Š PPO Step 2: Loss = 3.5967
  âœ… PPO Status: trained_successfully

ğŸŸ¢ Testing DPO (Simplified Implementation):
    ğŸ“Š DPO Step 0: Loss = 0.7121 (chosen: 3.9052, rejected: 3.5300)
    ğŸ“Š DPO Step 1: Loss = 0.7295 (chosen: 3.7420, rejected: 3.0282)
INFO:httpx:HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
    ğŸ“Š DPO Step 2: Loss = 0.7092 (chosen: 3.3758, rejected: 3.0580)
  âœ… DPO Status: trained_successfully

ğŸŸ£ Testing GRPO (Simplified Implementation):
    ğŸ“Š GRPO Step 0: Loss = 5.4993 (advantage: 5.326)
    ğŸ“Š GRPO Step 1: Loss = 3.9687 (advantage: 2.433)
    ğŸ“Š GRPO Step 2: Loss = 4.7474 (advantage: 3.613)
  âœ… GRPO Status: trained_successfully

ğŸ’¾ STEP 9: SAVING RESULTS
Saving all artifacts and analysis...
âœ… All results saved!

âœ… ALIGNMENT COMPARISON COMPLETE!
================================================================================
âœ… Methods demonstrated: PPO, DPO, GRPO
âœ… Base model: gpt2
âœ… Datasets processed: 2
âœ… Evaluation completed: âœ…
âœ… Interactive demos: âœ…
âœ… Training test: âœ…
================================================================================

ğŸ§ª TRAINING TEST RESULTS:
  â€¢ PPO: âœ… Trained successfully (3 steps, final loss: 3.5967)
  â€¢ DPO: âœ… Trained successfully (3 steps, final loss: 0.7092)
  â€¢ GRPO: âœ… Trained successfully (3 steps, final loss: 4.7474)
================================================================================
ğŸ“ Learning Outcomes:
  â€¢ Understanding of PPO, DPO, and GRPO concepts
  â€¢ Hands-on preference data collection
  â€¢ Method comparison and evaluation
  â€¢ Interactive demonstration tools
  â€¢ Best practice recommendations
================================================================================

ğŸ† BEST METHOD: GRPO
ğŸ“Š Score: 0.480

ğŸ’¡ Key Insights:
  â€¢ DPO provides best balance of performance and simplicity
  â€¢ GRPO shows promise for complex reasoning tasks
  â€¢ PPO remains essential for safety-critical applications
  â€¢ Base models lack consistency for production use

ğŸš€ To launch demos:
  annotation_demo.launch()  # Preference collection
  comparison_demo.launch()  # Method comparison

ğŸ‰ SUCCESS! Class 7 completed successfully!
ğŸ“Š Best method: grpo
