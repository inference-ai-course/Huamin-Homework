import os
import re
import requests
import feedparser
import fitz  # PyMuPDF
import faiss
import numpy as np
from pathlib import Path
from typing import List, Dict
from sentence_transformers import SentenceTransformer

# -----------------------------
# 0ï¸âƒ£ Global Configurations
# -----------------------------
BASE_URL = "http://export.arxiv.org/api/query?"
QUERY = "search_query=cat:cs.CL&start=0&max_results=50&sortBy=submittedDate&sortOrder=descending"

PDF_DIR = Path("arxiv_pdfs")
TXT_DIR = Path("arxiv_texts")
PDF_DIR.mkdir(exist_ok=True)
TXT_DIR.mkdir(exist_ok=True)

MODEL_NAME = "all-MiniLM-L6-v2"

# -----------------------------
# 1ï¸âƒ£ Download PDF
# -----------------------------
def safe_filename(title: str, max_len=80) -> str:
    name = re.sub(r'[\\/*?:"<>|]', "_", title)
    return name[:max_len]

def download_pdfs():
    feed = feedparser.parse(BASE_URL + QUERY)
    print("æ‰¾åˆ°è®ºæ–‡æ•°é‡:", len(feed.entries))

    for entry in feed.entries:
        pdf_url = entry.id.replace("abs", "pdf") + ".pdf"
        title = safe_filename(entry.title.replace(" ", "_"))
        filename = PDF_DIR / f"{title}.pdf"

        if not filename.exists():
            try:
                r = requests.get(pdf_url)
                with open(filename, "wb") as f:
                    f.write(r.content)
                print("å·²ä¸‹è½½:", filename.name)
            except Exception as e:
                print("ä¸‹è½½å¤±è´¥:", pdf_url, e)

# -----------------------------
# 2ï¸âƒ£ PDF â†’ Text
# -----------------------------
def extract_text_from_pdf(pdf_path: Path) -> str:
    doc = fitz.open(pdf_path)
    texts = []
    for page in doc:
        text = page.get_text("text").strip()
        if text:
            texts.append(text)
    full_text = "\n".join(texts)
    return re.sub(r"\s+", " ", full_text)

def save_texts():
    documents = []
    for pdf in PDF_DIR.glob("*.pdf"):
        raw_text = extract_text_from_pdf(pdf)
        if raw_text:
            documents.append(raw_text)
            out_file = TXT_DIR / f"{pdf.stem}.txt"
            with open(out_file, "w", encoding="utf-8") as f:
                f.write(raw_text)
            print(f"æŠ½å–å®Œæˆ: {pdf.name}, æ–‡æœ¬é•¿åº¦: {len(raw_text)}")
    return documents

# -----------------------------
# 3ï¸âƒ£ Chunking Text
# -----------------------------
def chunk_text(doc_id: str, text: str, chunk_size=500, overlap=100) -> List[Dict]:
    words = text.split()
    chunks = []
    i = 0
    chunk_id = 0
    while i < len(words):
        chunk_words = words[i:i + chunk_size]
        chunk_text = " ".join(chunk_words)
        chunks.append({
            "doc_id": doc_id,
            "chunk_id": chunk_id,
            "text": chunk_text
        })
        i += chunk_size - overlap
        chunk_id += 1
    return chunks

# -----------------------------
# 4ï¸âƒ£ Vector Indexing
# -----------------------------
def build_index(chunks: List[Dict]):
    """ç”Ÿæˆå‘é‡å¹¶å»ºç«‹ FAISS ç´¢å¼•"""
    texts = [c["text"] for c in chunks]
    model = SentenceTransformer(MODEL_NAME)
    embeddings = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)

    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    return index, embeddings, chunks, model

# -----------------------------
# 5ï¸âƒ£ Query
# -----------------------------
def search(query: str, index, model, chunks, top_k=3):
    """æŸ¥è¯¢å¹¶è¿”å›žæœ€ç›¸å…³çš„æ–‡æœ¬å—"""
    q_emb = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
    D, I = index.search(q_emb, top_k)

    results = []
    for idx in I[0]:
        results.append(chunks[idx])
    return results

# -----------------------------
# Main
# -----------------------------
if __name__ == "__main__":
    # Step 1: ä¸‹è½½è®ºæ–‡
    download_pdfs()

    # Step 2: PDF è½¬æ–‡æœ¬
    documents = save_texts()

    # Step 3: åˆ‡å—
    all_chunks = []
    for pdf_file in PDF_DIR.glob("*.pdf"):
        doc_id = pdf_file.stem
        text = extract_text_from_pdf(pdf_file)
        chunks = chunk_text(doc_id, text)
        all_chunks.extend(chunks)
    print(f"Total chunks: {len(all_chunks)}")

    # Step 4: å»ºç«‹ç´¢å¼•
    index, embeddings, chunks, model = build_index(all_chunks)

    # Step 5: æµ‹è¯•æŸ¥è¯¢
    query = "What are recent methods in machine translation?"
    results = search(query, index, model, chunks)

    print("\nðŸ” Search Results:")
    for r in results:
        print(f"- Doc: {r['doc_id']}, Chunk: {r['chunk_id']}, Text: {r['text'][:200]}...")
