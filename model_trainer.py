# model_trainer.py
import json
import torch
from pathlib import Path
from datasets import load_dataset

def check_gpu():
    """æ£€æŸ¥GPUå¯ç”¨æ€§"""
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"âœ… GPUå¯ç”¨: {gpu_name}")
        print(f"   GPUæ•°é‡: {gpu_count}")
        print(f"   æ˜¾å­˜: {gpu_memory:.1f} GB")
        return True
    else:
        print("âŒ æ²¡æœ‰æ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUï¼ˆä¸æŽ¨èï¼‰")
        return False

def real_fine_tuning(data_file: str = "data/synthetic_qa.jsonl"):
    """çœŸå®žçš„QLoRAå¾®è°ƒ"""
    print("ðŸ¤– å¼€å§‹çœŸå®žæ¨¡åž‹å¾®è°ƒ...")
    
    # æ£€æŸ¥GPU
    if not check_gpu():
        return {"status": "error", "message": "No GPU available"}
    
    try:
        from unsloth import FastLanguageModel
        from transformers import TrainingArguments
        from trl import SFTTrainer
        import transformers
        transformers.logging.set_verbosity_error()
        
        print("ðŸ”§ é…ç½®æ¨¡åž‹å‚æ•°...")
        
        # æ¨¡åž‹é…ç½® - ä½¿ç”¨4bité‡åŒ–èŠ‚çœæ˜¾å­˜
        model_name = "unsloth/llama-3-8b-bnb-4bit"
        # æˆ–è€…ä½¿ç”¨æ›´å°çš„æ¨¡åž‹ï¼š"unsloth/llama-3-7b-bnb-4bit"
        
        # åŠ è½½æ¨¡åž‹ï¼ˆè¿™æ­¥éœ€è¦GPUï¼‰
        print("ðŸ“¦ åŠ è½½4-bité‡åŒ–æ¨¡åž‹...")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = model_name,
            max_seq_length = 2048,  # å¯ä»¥å‡å°ä»¥èŠ‚çœå†…å­˜
            load_in_4bit = True,
            # token = "hf_...",  # å¦‚æžœéœ€è¦HuggingFace token
        )
        
        # å‡†å¤‡æ¨¡åž‹ç”¨äºŽè®­ç»ƒ
        model = FastLanguageModel.get_peft_model(
            model,
            r = 16,  # LoRA rank
            lora_alpha = 32,  # LoRA alpha
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                             "gate_proj", "up_proj", "down_proj"],
            lora_dropout = 0.05,
            bias = "none",
            use_gradient_checkpointing = True,
            random_state = 3407,
            use_rslora = False,
            loftq_config = None,
        )
        
        # åŠ è½½æ•°æ®é›†
        print("ðŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
        dataset = load_dataset("json", data_files=data_file, split="train")
        
        if len(dataset) == 0:
            return {"status": "error", "message": "No training data found"}
        
        print(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(dataset)}")
        
        # é…ç½®è®­ç»ƒå‚æ•° - é’ˆå¯¹16G GPUä¼˜åŒ–
        training_args = TrainingArguments(
            output_dir = "./llama3-8b-academic-qa",
            per_device_train_batch_size = 2,  # æ ¹æ®æ˜¾å­˜è°ƒæ•´
            gradient_accumulation_steps = 4,
            warmup_steps = 5,
            num_train_epochs = 2,  # å¯ä»¥å¢žåŠ åˆ°3-4ä¸ªepoch
            learning_rate = 2e-4,
            fp16 = not torch.cuda.is_bf16_supported(),
            bf16 = torch.cuda.is_bf16_supported(),
            logging_steps = 1,
            optim = "adamw_8bit",
            weight_decay = 0.01,
            lr_scheduler_type = "linear",
            seed = 3407,
            save_strategy = "epoch",
            report_to = None,
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SFTTrainer(
            model = model,
            tokenizer = tokenizer,
            train_dataset = dataset,
            dataset_text_field = "text",
            max_seq_length = 1024,
            args = training_args,
        )
        
        print("ðŸ”„ å¼€å§‹è®­ç»ƒå¾ªçŽ¯ï¼ˆè¿™éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
        print("   æ³¨æ„ï¼šè®­ç»ƒæ—¶é—´å–å†³äºŽæ•°æ®é‡å’ŒGPUæ€§èƒ½")
        print("   é¢„è®¡æ—¶é—´: 10-30åˆ†é’Ÿ")
        
        # å¼€å§‹è®­ç»ƒï¼
        trainer.train()
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
        # ä¿å­˜æ¨¡åž‹
        print("ðŸ’¾ ä¿å­˜æ¨¡åž‹...")
        trainer.save_model()
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯
        model_info = {
            "model_name": model_name,
            "training_samples": len(dataset),
            "epochs": training_args.num_train_epochs,
            "batch_size": training_args.per_device_train_batch_size,
            "learning_rate": training_args.learning_rate,
            "status": "success",
            "gpu_used": torch.cuda.get_device_name(0),
            "training_time": "æŸ¥çœ‹æ—¥å¿—èŽ·å–å…·ä½“æ—¶é—´",
        }
        
        with open("data/training_info.json", "w", encoding="utf-8") as f:
            json.dump(model_info, f, indent=2, ensure_ascii=False)
        
        return model_info
        
    except Exception as e:
        error_msg = f"è®­ç»ƒé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")
        import traceback
        traceback.print_exc()
        return {"status": "error", "message": error_msg}

def mock_fine_tuning(data_file: str = "data/synthetic_qa.jsonl"):
    """æ¨¡æ‹Ÿå¾®è°ƒï¼ˆå¤‡ç”¨ï¼‰"""
    print("âš ï¸  ä½¿ç”¨æ¨¡æ‹Ÿè®­ç»ƒï¼ˆGPUä¸å¯ç”¨ï¼‰")
    # ... ä¿æŒåŽŸæœ‰çš„æ¨¡æ‹Ÿä»£ç  ...

# è‡ªåŠ¨é€‰æ‹©è®­ç»ƒæ¨¡å¼
def fine_tune_model(data_file: str = "data/synthetic_qa.jsonl"):
    """è‡ªåŠ¨é€‰æ‹©è®­ç»ƒæ¨¡å¼"""
    if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory >= 10 * 1024**3:
        return real_fine_tuning(data_file)
    else:
        return mock_fine_tuning(data_file)

if __name__ == "__main__":
    fine_tune_model()