{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529efc50-535a-4e47-9b56-1d6b7b4eb1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run RAG_pipeline_ollama.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee53f3e-e0dd-4c1e-9bda-8f3efa5bc399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Document 1 content preview:Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external ..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split documents into 2 chunks.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Chunk 1:Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.It enhances the capabilities of language models by providing them with access to external knowledge sources.\n",
       "\n",
       "Text embeddings are numerical representations of text that capture semantic meaning.They are used to measure similarity between pieces of text.\n",
       "\n",
       "Vector databases store these embeddings and allow for efficient similarity searches."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Chunk 2:LangChain is a framework that facilitates the development of applications powered by language models.It provides tools for integrating LLMs with external data sources.\n",
       "\n",
       "Evaluation metrics for RAG systems include precision, recall, and F1 score, which assess the quality of the generated responses."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vectorstore created with local Ollama embeddings.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Vectorstore Contents:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Document 1: Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation.It enhances the capabilities of language models by providing them with access to external knowledge sources.\n",
       "\n",
       "Text embeddings are numerical representations of text that capture semantic meaning.They are used to measure similarity between pieces of text.\n",
       "\n",
       "Vector databases store these embeddings and allow for efficient similarity searches."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Document 2: LangChain is a framework that facilitates the development of applications powered by language models.It provides tools for integrating LLMs with external data sources.\n",
       "\n",
       "Evaluation metrics for RAG systems include precision, recall, and F1 score, which assess the quality of the generated responses."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever set up from vectorstore.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Retriever details: tags=['Chroma', 'OllamaEmbeddings'] vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x116677510> search_kwargs={}"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ QA chain initialized with local Ollama model: llama3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "QA chain details: verbose=False combine_documents_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=OllamaLLM(model='llama3', temperature=0.0), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') retriever=VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x116677510>, search_kwargs={})"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query: What is Retrieval-Augmented Generation?<br>Answer: According to the provided context, Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Test Query Answer:</h3>According to the provided context, Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query 1: Define RAG.<br>Expected: Retrieval-Augmented Generation<br>Model Answer: According to the provided context, Retrieval-Augmented Generation (RAG) is a technique that combines information retrieval with text generation. It enhances the capabilities of language models by providing them with access to external knowledge sources."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query 2: Explain vector databases.<br>Expected: Vector databases store embeddings<br>Model Answer: According to the provided context, a vector database stores text embeddings (numerical representations of text that capture semantic meaning) and allows for efficient similarity searches."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Evaluation Accuracy: 50.00%"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Evaluation Accuracy: 50.00%</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run RAG_main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70adf4-00fc-4213-9973-f7390da7c36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
